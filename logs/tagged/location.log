2022-07-11 21:13:13,973 - INFO - allennlp.common.params - random_seed = 13370
2022-07-11 21:13:13,974 - INFO - allennlp.common.params - numpy_seed = 1337
2022-07-11 21:13:13,974 - INFO - allennlp.common.params - pytorch_seed = 133
2022-07-11 21:13:14,031 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu113
2022-07-11 21:13:14,031 - INFO - allennlp.common.params - type = default
2022-07-11 21:13:14,032 - INFO - allennlp.common.params - dataset_reader.type = classification-tsv
2022-07-11 21:13:14,032 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-07-11 21:13:14,032 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-07-11 21:13:14,032 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2022-07-11 21:13:14,032 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2022-07-11 21:13:14,033 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = DeepPavlov/rubert-base-cased
2022-07-11 21:13:14,033 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2022-07-11 21:13:14,033 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None
2022-07-11 21:13:14,033 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2022-07-11 21:13:14,033 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None
2022-07-11 21:13:21,586 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = pretrained_transformer
2022-07-11 21:13:21,586 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.token_min_padding_length = 0
2022-07-11 21:13:21,586 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.model_name = DeepPavlov/rubert-base-cased
2022-07-11 21:13:21,586 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.namespace = tags
2022-07-11 21:13:21,586 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_length = None
2022-07-11 21:13:21,586 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.tokenizer_kwargs = None
2022-07-11 21:13:21,587 - INFO - allennlp.common.params - dataset_reader.max_tokens = 512
2022-07-11 21:13:21,588 - INFO - allennlp.common.params - train_data_path = /content/tagged_data/location_train.tsv
2022-07-11 21:13:21,588 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f035e27e950>
2022-07-11 21:13:21,588 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-07-11 21:13:21,588 - INFO - allennlp.common.params - validation_dataset_reader = None
2022-07-11 21:13:21,588 - INFO - allennlp.common.params - validation_data_path = /content/tagged_data/location_test.tsv
2022-07-11 21:13:21,588 - INFO - allennlp.common.params - validation_data_loader = None
2022-07-11 21:13:21,588 - INFO - allennlp.common.params - test_data_path = None
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - evaluate_on_test = False
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - batch_weight_key = 
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - data_loader.batch_size = 8
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - data_loader.shuffle = True
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2022-07-11 21:13:21,589 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-07-11 21:13:21,590 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-07-11 21:13:21,590 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-07-11 21:13:21,590 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-07-11 21:13:21,590 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-07-11 21:13:21,590 - INFO - allennlp.common.params - data_loader.quiet = False
2022-07-11 21:13:21,590 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f0364f72c10>
2022-07-11 21:13:21,590 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-07-11 21:13:31,653 - INFO - tqdm - loading instances: 2869it [00:10, 346.27it/s]
2022-07-11 21:13:35,850 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.batch_size = 8
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.shuffle = True
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.quiet = False
2022-07-11 21:13:35,851 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f0364f72c10>
2022-07-11 21:13:35,852 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-07-11 21:13:41,373 - INFO - allennlp.common.params - type = from_instances
2022-07-11 21:13:41,374 - INFO - allennlp.common.params - min_count = None
2022-07-11 21:13:41,374 - INFO - allennlp.common.params - max_vocab_size = None
2022-07-11 21:13:41,374 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-07-11 21:13:41,374 - INFO - allennlp.common.params - pretrained_files = None
2022-07-11 21:13:41,374 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-07-11 21:13:41,374 - INFO - allennlp.common.params - tokens_to_add = None
2022-07-11 21:13:41,375 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-07-11 21:13:41,375 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-07-11 21:13:41,375 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-07-11 21:13:41,375 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-07-11 21:13:41,375 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-07-11 21:13:41,468 - INFO - allennlp.common.params - model.type = simple_classifier
2022-07-11 21:13:41,469 - INFO - allennlp.common.params - model.embedder.type = basic
2022-07-11 21:13:41,469 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.type = pretrained_transformer
2022-07-11 21:13:41,469 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.model_name = DeepPavlov/rubert-base-cased
2022-07-11 21:13:41,469 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.max_length = None
2022-07-11 21:13:41,469 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.sub_module = None
2022-07-11 21:13:41,469 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.train_parameters = True
2022-07-11 21:13:41,469 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.eval_mode = False
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.last_layer_only = True
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.override_weights_file = None
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.override_weights_strip_prefix = None
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.reinit_modules = None
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.load_weights = True
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.gradient_checkpointing = None
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.tokenizer_kwargs = None
2022-07-11 21:13:41,470 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.transformer_kwargs = None
2022-07-11 21:13:54,511 - INFO - allennlp.common.params - model.encoder.type = bert_pooler
2022-07-11 21:13:54,512 - INFO - allennlp.common.params - model.encoder.pretrained_model = DeepPavlov/rubert-base-cased
2022-07-11 21:13:54,512 - INFO - allennlp.common.params - model.encoder.override_weights_file = None
2022-07-11 21:13:54,512 - INFO - allennlp.common.params - model.encoder.override_weights_strip_prefix = None
2022-07-11 21:13:54,512 - INFO - allennlp.common.params - model.encoder.load_weights = True
2022-07-11 21:13:54,512 - INFO - allennlp.common.params - model.encoder.requires_grad = True
2022-07-11 21:13:54,512 - INFO - allennlp.common.params - model.encoder.dropout = 0.0
2022-07-11 21:13:54,512 - INFO - allennlp.common.params - model.encoder.transformer_kwargs = None
2022-07-11 21:13:54,864 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-07-11 21:13:54,865 - INFO - allennlp.common.params - trainer.cuda_device = None
2022-07-11 21:13:54,865 - INFO - allennlp.common.params - trainer.distributed = False
2022-07-11 21:13:54,865 - INFO - allennlp.common.params - trainer.world_size = 1
2022-07-11 21:13:54,865 - INFO - allennlp.common.params - trainer.patience = None
2022-07-11 21:13:54,865 - INFO - allennlp.common.params - trainer.validation_metric = -loss
2022-07-11 21:13:54,865 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-07-11 21:13:54,865 - INFO - allennlp.common.params - trainer.grad_norm = False
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.use_amp = False
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.no_grad = None
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.moving_average = None
2022-07-11 21:13:54,866 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f035e2be790>
2022-07-11 21:13:54,867 - INFO - allennlp.common.params - trainer.callbacks = None
2022-07-11 21:13:54,867 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2022-07-11 21:13:54,867 - INFO - allennlp.common.params - trainer.run_confidence_checks = True
2022-07-11 21:13:54,867 - INFO - allennlp.common.params - trainer.grad_scaling = True
2022-07-11 21:14:08,396 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-07-11 21:14:08,396 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-07-11 21:14:08,396 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-07-11 21:14:08,397 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)
2022-07-11 21:14:08,397 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08
2022-07-11 21:14:08,397 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0
2022-07-11 21:14:08,397 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-07-11 21:14:08,397 - INFO - allennlp.training.optimizers - Number of trainable parameters: 178445570
2022-07-11 21:14:08,404 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-07-11 21:14:08,405 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-07-11 21:14:08,405 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.word_embeddings.weight
2022-07-11 21:14:08,405 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.position_embeddings.weight
2022-07-11 21:14:08,405 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.token_type_embeddings.weight
2022-07-11 21:14:08,405 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.LayerNorm.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.LayerNorm.bias
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.query.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.query.bias
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.key.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.key.bias
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.value.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.value.bias
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.dense.weight
2022-07-11 21:14:08,406 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.dense.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.query.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.query.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.key.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.key.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.value.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.value.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.dense.weight
2022-07-11 21:14:08,407 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.dense.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.query.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.query.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.key.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.key.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.value.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.value.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.dense.weight
2022-07-11 21:14:08,408 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.dense.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.query.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.query.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.key.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.key.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.value.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.value.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.dense.weight
2022-07-11 21:14:08,409 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.dense.bias
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.query.weight
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.query.bias
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.key.weight
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.key.bias
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.value.weight
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.value.bias
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-07-11 21:14:08,410 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.dense.weight
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.dense.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.query.weight
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.query.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.key.weight
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.key.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.value.weight
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.value.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-07-11 21:14:08,411 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.dense.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.dense.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.query.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.query.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.key.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.key.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.value.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.value.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-07-11 21:14:08,412 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-07-11 21:14:08,413 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-07-11 21:14:08,413 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-07-11 21:14:08,413 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-07-11 21:14:08,413 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.dense.weight
2022-07-11 21:14:08,413 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.dense.bias
2022-07-11 21:14:08,483 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-07-11 21:14:08,483 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-07-11 21:14:08,483 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.query.weight
2022-07-11 21:14:08,483 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.query.bias
2022-07-11 21:14:08,483 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.key.weight
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.key.bias
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.value.weight
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.value.bias
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.dense.weight
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.dense.bias
2022-07-11 21:14:08,484 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.query.weight
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.query.bias
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.key.weight
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.key.bias
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.value.weight
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.value.bias
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-07-11 21:14:08,485 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.dense.weight
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.dense.bias
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.query.weight
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.query.bias
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.key.weight
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.key.bias
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.value.weight
2022-07-11 21:14:08,486 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.value.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.dense.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.dense.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.query.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.query.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.key.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.key.bias
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.value.weight
2022-07-11 21:14:08,487 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.value.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.dense.weight
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.dense.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.query.weight
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.query.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.key.weight
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.key.bias
2022-07-11 21:14:08,488 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.value.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.value.bias
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.dense.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.dense.bias
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.pooler.dense.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.pooler.dense.bias
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - encoder.pooler.dense.weight
2022-07-11 21:14:08,489 - INFO - allennlp.common.util - encoder.pooler.dense.bias
2022-07-11 21:14:08,490 - INFO - allennlp.common.util - classifier.weight
2022-07-11 21:14:08,490 - INFO - allennlp.common.util - classifier.bias
2022-07-11 21:14:08,490 - INFO - allennlp.common.params - type = default
2022-07-11 21:14:08,490 - INFO - allennlp.common.params - save_completed_epochs = True
2022-07-11 21:14:08,490 - INFO - allennlp.common.params - save_every_num_seconds = None
2022-07-11 21:14:08,490 - INFO - allennlp.common.params - save_every_num_batches = None
2022-07-11 21:14:08,490 - INFO - allennlp.common.params - keep_most_recent_by_count = 2
2022-07-11 21:14:08,490 - INFO - allennlp.common.params - keep_most_recent_by_age = None
2022-07-11 21:14:08,491 - WARNING - allennlp.training.gradient_descent_trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-07-11 21:14:08,492 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.
2022-07-11 21:14:08,492 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/9
2022-07-11 21:14:08,492 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.3G
2022-07-11 21:14:08,494 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 681M
2022-07-11 21:14:08,495 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 21:14:08,495 - INFO - tqdm - 0%|          | 0/549 [00:00<?, ?it/s]
2022-07-11 21:14:08,936 - INFO - allennlp.training.callbacks.console_logger - Batch inputs
2022-07-11 21:14:08,936 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/token_ids (Shape: 8 x 448)
tensor([[   101,  43103,  40174,  ...,      0,      0,      0],
        [   101,  67082,   6329,  ...,      0,      0,      0],
        [   101,   4752,   3332,  ...,      0,      0,      0],
        ...,
        [   101, 113168,    852,  ...,      0,      0,      0],
        [   101,    108,    869,  ...,    132,    108,    102],
        [   101,   3099,  50379,  ...,      0,      0,      0]],
       device='cuda:0')
2022-07-11 21:14:08,942 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/mask (Shape: 8 x 448)
tensor([[ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        ...,
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')
2022-07-11 21:14:08,944 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/type_ids (Shape: 8 x 448)
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')
2022-07-11 21:14:08,946 - INFO - allennlp.training.callbacks.console_logger - batch_input/label (Shape: 8)
tensor([0, 1, 1,  ..., 0, 1, 0], device='cuda:0')
2022-07-11 21:14:18,650 - INFO - tqdm - accuracy: 0.5286, batch_loss: 0.7552, loss: 0.6978 ||:   6%|6         | 35/549 [00:10<02:31,  3.40it/s]
2022-07-11 21:14:28,896 - INFO - tqdm - accuracy: 0.5199, batch_loss: 0.6887, loss: 0.6925 ||:  13%|#2        | 69/549 [00:20<02:52,  2.78it/s]
2022-07-11 21:14:39,230 - INFO - tqdm - accuracy: 0.5309, batch_loss: 0.6147, loss: 0.6890 ||:  18%|#8        | 101/549 [00:30<02:40,  2.80it/s]
2022-07-11 21:14:49,441 - INFO - tqdm - accuracy: 0.5360, batch_loss: 0.6296, loss: 0.6862 ||:  24%|##4       | 132/549 [00:40<02:18,  3.01it/s]
2022-07-11 21:14:59,687 - INFO - tqdm - accuracy: 0.5508, batch_loss: 0.6384, loss: 0.6815 ||:  30%|###       | 165/549 [00:51<02:09,  2.97it/s]
2022-07-11 21:15:09,819 - INFO - tqdm - accuracy: 0.5754, batch_loss: 0.7825, loss: 0.6669 ||:  36%|###6      | 199/549 [01:01<02:00,  2.92it/s]
2022-07-11 21:15:19,837 - INFO - tqdm - accuracy: 0.5803, batch_loss: 0.7835, loss: 0.6664 ||:  42%|####2     | 232/549 [01:11<01:24,  3.73it/s]
2022-07-11 21:15:30,027 - INFO - tqdm - accuracy: 0.5881, batch_loss: 0.3822, loss: 0.6591 ||:  48%|####8     | 264/549 [01:21<01:42,  2.77it/s]
2022-07-11 21:15:40,317 - INFO - tqdm - accuracy: 0.5985, batch_loss: 0.4024, loss: 0.6512 ||:  54%|#####4    | 297/549 [01:31<01:38,  2.57it/s]
2022-07-11 21:15:50,504 - INFO - tqdm - accuracy: 0.6045, batch_loss: 0.5125, loss: 0.6479 ||:  60%|######    | 330/549 [01:42<00:47,  4.57it/s]
2022-07-11 21:16:00,520 - INFO - tqdm - accuracy: 0.6098, batch_loss: 0.8074, loss: 0.6421 ||:  66%|######6   | 363/549 [01:52<01:00,  3.10it/s]
2022-07-11 21:16:10,641 - INFO - tqdm - accuracy: 0.6225, batch_loss: 0.5467, loss: 0.6305 ||:  72%|#######2  | 396/549 [02:02<00:39,  3.84it/s]
2022-07-11 21:16:20,732 - INFO - tqdm - accuracy: 0.6279, batch_loss: 0.6529, loss: 0.6270 ||:  79%|#######8  | 431/549 [02:12<00:27,  4.29it/s]
2022-07-11 21:16:30,854 - INFO - tqdm - accuracy: 0.6344, batch_loss: 0.7755, loss: 0.6187 ||:  85%|########4 | 466/549 [02:22<00:28,  2.93it/s]
2022-07-11 21:16:40,959 - INFO - tqdm - accuracy: 0.6390, batch_loss: 0.7142, loss: 0.6160 ||:  91%|#########1| 501/549 [02:32<00:09,  4.86it/s]
2022-07-11 21:16:51,023 - INFO - tqdm - accuracy: 0.6431, batch_loss: 0.4209, loss: 0.6127 ||:  97%|#########6| 532/549 [02:42<00:05,  3.11it/s]
2022-07-11 21:16:55,695 - INFO - tqdm - accuracy: 0.6479, batch_loss: 0.7823, loss: 0.6086 ||: 100%|#########9| 547/549 [02:47<00:00,  3.22it/s]
2022-07-11 21:16:56,014 - INFO - tqdm - accuracy: 0.6478, batch_loss: 0.9061, loss: 0.6092 ||: 100%|#########9| 548/549 [02:47<00:00,  3.19it/s]
2022-07-11 21:16:56,090 - INFO - tqdm - accuracy: 0.6480, batch_loss: 0.4010, loss: 0.6088 ||: 100%|##########| 549/549 [02:47<00:00,  3.28it/s]
2022-07-11 21:16:56,090 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 21:16:56,091 - INFO - tqdm - 0%|          | 0/197 [00:00<?, ?it/s]
2022-07-11 21:16:56,190 - INFO - allennlp.training.callbacks.console_logger - Batch inputs
2022-07-11 21:16:56,191 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/token_ids (Shape: 8 x 300)
tensor([[  101,  6351, 48534,  ..., 90453,   132,   102],
        [  101, 91814,  2648,  ...,     0,     0,     0],
        [  101,   108, 58615,  ...,     0,     0,     0],
        ...,
        [  101,   108,   781,  ...,     0,     0,     0],
        [  101,  6351, 16467,  ...,     0,     0,     0],
        [  101, 11908,   869,  ...,     0,     0,     0]], device='cuda:0')
2022-07-11 21:16:56,193 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/mask (Shape: 8 x 300)
tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        ...,
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')
2022-07-11 21:16:56,195 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/type_ids (Shape: 8 x 300)
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')
2022-07-11 21:16:56,197 - INFO - allennlp.training.callbacks.console_logger - batch_input/label (Shape: 8)
tensor([0, 0, 0,  ..., 1, 0, 1], device='cuda:0')
2022-07-11 21:17:06,207 - INFO - tqdm - accuracy: 0.7913, batch_loss: 0.3492, loss: 0.4705 ||:  57%|#####6    | 112/197 [00:10<00:07, 11.29it/s]
2022-07-11 21:17:14,131 - INFO - tqdm - accuracy: 0.7798, batch_loss: 0.5587, loss: 0.4808 ||: 100%|##########| 197/197 [00:18<00:00, 10.92it/s]
2022-07-11 21:17:14,132 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 21:17:14,132 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.648  |     0.780
2022-07-11 21:17:14,132 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |   680.989  |       N/A
2022-07-11 21:17:14,132 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.609  |     0.481
2022-07-11 21:17:14,132 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5466.145  |       N/A
2022-07-11 21:17:22,867 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:03:14.375025
2022-07-11 21:17:22,870 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:27:50
2022-07-11 21:17:22,870 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/9
2022-07-11 21:17:22,870 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.7G
2022-07-11 21:17:22,870 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 21:17:22,871 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 21:17:22,872 - INFO - tqdm - 0%|          | 0/549 [00:00<?, ?it/s]
2022-07-11 21:17:33,144 - INFO - tqdm - accuracy: 0.8229, batch_loss: 0.6185, loss: 0.4038 ||:   7%|6         | 36/549 [00:10<02:43,  3.13it/s]
2022-07-11 21:17:43,462 - INFO - tqdm - accuracy: 0.8290, batch_loss: 0.4407, loss: 0.3851 ||:  12%|#2        | 68/549 [00:20<02:37,  3.06it/s]
2022-07-11 21:17:53,563 - INFO - tqdm - accuracy: 0.7983, batch_loss: 0.2174, loss: 0.4223 ||:  18%|#8        | 101/549 [00:30<02:15,  3.32it/s]
2022-07-11 21:18:03,579 - INFO - tqdm - accuracy: 0.7886, batch_loss: 0.3610, loss: 0.4351 ||:  25%|##4       | 136/549 [00:40<01:52,  3.67it/s]
2022-07-11 21:18:13,597 - INFO - tqdm - accuracy: 0.7897, batch_loss: 0.6627, loss: 0.4459 ||:  31%|###       | 170/549 [00:50<01:31,  4.13it/s]
2022-07-11 21:18:23,652 - INFO - tqdm - accuracy: 0.7964, batch_loss: 0.4976, loss: 0.4391 ||:  37%|###6      | 202/549 [01:00<01:47,  3.23it/s]
2022-07-11 21:18:33,884 - INFO - tqdm - accuracy: 0.7957, batch_loss: 0.2813, loss: 0.4406 ||:  43%|####2     | 235/549 [01:11<01:45,  2.97it/s]
2022-07-11 21:18:43,907 - INFO - tqdm - accuracy: 0.7979, batch_loss: 0.5029, loss: 0.4430 ||:  49%|####8     | 269/549 [01:21<01:25,  3.26it/s]
2022-07-11 21:18:54,207 - INFO - tqdm - accuracy: 0.7954, batch_loss: 0.6582, loss: 0.4437 ||:  55%|#####4    | 300/549 [01:31<01:29,  2.77it/s]
2022-07-11 21:19:04,372 - INFO - tqdm - accuracy: 0.7997, batch_loss: 0.2964, loss: 0.4401 ||:  60%|######    | 332/549 [01:41<00:53,  4.08it/s]
2022-07-11 21:19:14,454 - INFO - tqdm - accuracy: 0.8003, batch_loss: 0.3136, loss: 0.4370 ||:  66%|######6   | 363/549 [01:51<00:54,  3.43it/s]
2022-07-11 21:19:24,861 - INFO - tqdm - accuracy: 0.7992, batch_loss: 0.6053, loss: 0.4376 ||:  72%|#######1  | 394/549 [02:01<00:55,  2.81it/s]
2022-07-11 21:19:34,925 - INFO - tqdm - accuracy: 0.7968, batch_loss: 0.7603, loss: 0.4392 ||:  78%|#######7  | 427/549 [02:12<00:41,  2.95it/s]
2022-07-11 21:19:45,059 - INFO - tqdm - accuracy: 0.7962, batch_loss: 0.5012, loss: 0.4409 ||:  83%|########3 | 457/549 [02:22<00:34,  2.65it/s]
2022-07-11 21:19:55,272 - INFO - tqdm - accuracy: 0.7924, batch_loss: 0.7156, loss: 0.4440 ||:  89%|########9 | 489/549 [02:32<00:22,  2.72it/s]
2022-07-11 21:20:05,516 - INFO - tqdm - accuracy: 0.7923, batch_loss: 0.3707, loss: 0.4429 ||:  95%|#########5| 523/549 [02:42<00:09,  2.70it/s]
2022-07-11 21:20:13,096 - INFO - tqdm - accuracy: 0.7918, batch_loss: 0.3590, loss: 0.4420 ||: 100%|#########9| 547/549 [02:50<00:00,  3.00it/s]
2022-07-11 21:20:13,257 - INFO - tqdm - accuracy: 0.7917, batch_loss: 0.5392, loss: 0.4422 ||: 100%|#########9| 548/549 [02:50<00:00,  3.55it/s]
2022-07-11 21:20:13,387 - INFO - tqdm - accuracy: 0.7916, batch_loss: 1.0943, loss: 0.4434 ||: 100%|##########| 549/549 [02:50<00:00,  4.23it/s]
2022-07-11 21:20:13,388 - INFO - tqdm - accuracy: 0.7916, batch_loss: 1.0943, loss: 0.4434 ||: 100%|##########| 549/549 [02:50<00:00,  3.22it/s]
2022-07-11 21:20:13,389 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 21:20:13,390 - INFO - tqdm - 0%|          | 0/197 [00:00<?, ?it/s]
2022-07-11 21:20:23,445 - INFO - tqdm - accuracy: 0.8090, batch_loss: 0.7932, loss: 0.4537 ||:  55%|#####4    | 108/197 [00:10<00:09,  9.14it/s]
2022-07-11 21:20:31,772 - INFO - tqdm - accuracy: 0.7951, batch_loss: 0.6736, loss: 0.4706 ||: 100%|##########| 197/197 [00:18<00:00, 11.82it/s]
2022-07-11 21:20:31,773 - INFO - tqdm - accuracy: 0.7951, batch_loss: 0.6736, loss: 0.4706 ||: 100%|##########| 197/197 [00:18<00:00, 10.72it/s]
2022-07-11 21:20:31,773 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 21:20:31,773 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.792  |     0.795
2022-07-11 21:20:31,773 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7820.602  |       N/A
2022-07-11 21:20:31,773 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.443  |     0.471
2022-07-11 21:20:31,773 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5830.316  |       N/A
2022-07-11 21:20:40,023 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:03:17.153481
2022-07-11 21:20:40,024 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:25:33
2022-07-11 21:20:40,024 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/9
2022-07-11 21:20:40,024 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.7G
2022-07-11 21:20:40,024 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 21:20:40,025 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 21:20:40,026 - INFO - tqdm - 0%|          | 0/549 [00:00<?, ?it/s]
2022-07-11 21:20:50,092 - INFO - tqdm - accuracy: 0.8857, batch_loss: 0.0740, loss: 0.2948 ||:   6%|6         | 35/549 [00:10<02:29,  3.45it/s]
2022-07-11 21:21:00,111 - INFO - tqdm - accuracy: 0.8929, batch_loss: 0.1279, loss: 0.3004 ||:  13%|#2        | 70/549 [00:20<02:25,  3.30it/s]
2022-07-11 21:21:10,123 - INFO - tqdm - accuracy: 0.8618, batch_loss: 0.4789, loss: 0.3368 ||:  19%|#8        | 104/549 [00:30<02:00,  3.70it/s]
2022-07-11 21:21:20,485 - INFO - tqdm - accuracy: 0.8741, batch_loss: 0.3781, loss: 0.3195 ||:  26%|##5       | 141/549 [00:40<02:03,  3.31it/s]
2022-07-11 21:21:30,535 - INFO - tqdm - accuracy: 0.8693, batch_loss: 0.1786, loss: 0.3278 ||:  32%|###1      | 174/549 [00:50<02:26,  2.56it/s]
2022-07-11 21:21:40,646 - INFO - tqdm - accuracy: 0.8677, batch_loss: 0.3107, loss: 0.3302 ||:  38%|###7      | 206/549 [01:00<02:03,  2.78it/s]
2022-07-11 21:21:50,886 - INFO - tqdm - accuracy: 0.8688, batch_loss: 0.3888, loss: 0.3233 ||:  44%|####3     | 241/549 [01:10<01:50,  2.79it/s]
2022-07-11 21:22:01,081 - INFO - tqdm - accuracy: 0.8676, batch_loss: 0.3433, loss: 0.3271 ||:  49%|####8     | 269/549 [01:21<01:50,  2.54it/s]
2022-07-11 21:22:11,426 - INFO - tqdm - accuracy: 0.8642, batch_loss: 0.3485, loss: 0.3306 ||:  55%|#####4    | 301/549 [01:31<01:31,  2.71it/s]
2022-07-11 21:22:21,682 - INFO - tqdm - accuracy: 0.8675, batch_loss: 1.3554, loss: 0.3251 ||:  61%|######    | 333/549 [01:41<01:22,  2.63it/s]
2022-07-11 21:22:31,803 - INFO - tqdm - accuracy: 0.8627, batch_loss: 0.1747, loss: 0.3306 ||:  67%|######6   | 367/549 [01:51<00:56,  3.23it/s]
2022-07-11 21:22:41,858 - INFO - tqdm - accuracy: 0.8668, batch_loss: 0.2468, loss: 0.3265 ||:  72%|#######2  | 398/549 [02:01<00:47,  3.17it/s]
2022-07-11 21:22:52,142 - INFO - tqdm - accuracy: 0.8666, batch_loss: 0.1785, loss: 0.3237 ||:  79%|#######8  | 432/549 [02:12<00:45,  2.58it/s]
2022-07-11 21:23:02,403 - INFO - tqdm - accuracy: 0.8669, batch_loss: 0.5928, loss: 0.3242 ||:  84%|########4 | 463/549 [02:22<00:29,  2.89it/s]
2022-07-11 21:23:12,416 - INFO - tqdm - accuracy: 0.8666, batch_loss: 0.5190, loss: 0.3242 ||:  90%|########9 | 494/549 [02:32<00:19,  2.80it/s]
2022-07-11 21:23:22,437 - INFO - tqdm - accuracy: 0.8670, batch_loss: 0.8933, loss: 0.3242 ||:  97%|#########6| 530/549 [02:42<00:05,  3.19it/s]
2022-07-11 21:23:27,450 - INFO - tqdm - accuracy: 0.8656, batch_loss: 0.1375, loss: 0.3237 ||: 100%|#########9| 547/549 [02:47<00:00,  2.93it/s]
2022-07-11 21:23:27,739 - INFO - tqdm - accuracy: 0.8656, batch_loss: 0.2204, loss: 0.3235 ||: 100%|#########9| 548/549 [02:47<00:00,  3.07it/s]
2022-07-11 21:23:27,884 - INFO - tqdm - accuracy: 0.8657, batch_loss: 0.2882, loss: 0.3234 ||: 100%|##########| 549/549 [02:47<00:00,  3.68it/s]
2022-07-11 21:23:27,884 - INFO - tqdm - accuracy: 0.8657, batch_loss: 0.2882, loss: 0.3234 ||: 100%|##########| 549/549 [02:47<00:00,  3.27it/s]
2022-07-11 21:23:27,884 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 21:23:27,885 - INFO - tqdm - 0%|          | 0/197 [00:00<?, ?it/s]
2022-07-11 21:23:37,940 - INFO - tqdm - accuracy: 0.7882, batch_loss: 0.6083, loss: 0.5104 ||:  55%|#####4    | 108/197 [00:10<00:08, 10.71it/s]
2022-07-11 21:23:45,916 - INFO - tqdm - accuracy: 0.7989, batch_loss: 0.5060, loss: 0.4898 ||: 100%|##########| 197/197 [00:18<00:00, 10.93it/s]
2022-07-11 21:23:45,917 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 21:23:45,917 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.866  |     0.799
2022-07-11 21:23:45,917 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7820.602  |       N/A
2022-07-11 21:23:45,917 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.323  |     0.490
2022-07-11 21:23:45,917 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5830.316  |       N/A
2022-07-11 21:23:54,250 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:03:14.226422
2022-07-11 21:23:54,250 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:22:27
2022-07-11 21:23:54,251 - INFO - allennlp.training.gradient_descent_trainer - Epoch 3/9
2022-07-11 21:23:54,251 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.7G
2022-07-11 21:23:54,251 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 21:23:54,252 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 21:23:54,252 - INFO - tqdm - 0%|          | 0/549 [00:00<?, ?it/s]
2022-07-11 21:24:04,485 - INFO - tqdm - accuracy: 0.9358, batch_loss: 0.4810, loss: 0.1895 ||:   7%|6         | 37/549 [00:10<02:27,  3.46it/s]
2022-07-11 21:24:14,492 - INFO - tqdm - accuracy: 0.9296, batch_loss: 0.1719, loss: 0.1939 ||:  13%|#2        | 71/549 [00:20<02:46,  2.87it/s]
2022-07-11 21:24:24,514 - INFO - tqdm - accuracy: 0.9327, batch_loss: 0.1451, loss: 0.1905 ||:  19%|#8        | 104/549 [00:30<02:15,  3.28it/s]
2022-07-11 21:24:34,819 - INFO - tqdm - accuracy: 0.9299, batch_loss: 0.0281, loss: 0.1901 ||:  25%|##5       | 139/549 [00:40<02:07,  3.21it/s]
2022-07-11 21:24:44,925 - INFO - tqdm - accuracy: 0.9287, batch_loss: 0.0292, loss: 0.1902 ||:  31%|###       | 170/549 [00:50<02:13,  2.85it/s]
2022-07-11 21:24:55,040 - INFO - tqdm - accuracy: 0.9276, batch_loss: 0.1214, loss: 0.1934 ||:  37%|###6      | 202/549 [01:00<01:43,  3.37it/s]
2022-07-11 21:25:05,115 - INFO - tqdm - accuracy: 0.9298, batch_loss: 0.0297, loss: 0.1915 ||:  43%|####2     | 235/549 [01:10<01:42,  3.07it/s]
2022-07-11 21:25:15,193 - INFO - tqdm - accuracy: 0.9330, batch_loss: 0.1049, loss: 0.1859 ||:  48%|####8     | 265/549 [01:20<01:40,  2.82it/s]
2022-07-11 21:25:25,340 - INFO - tqdm - accuracy: 0.9287, batch_loss: 0.3871, loss: 0.1978 ||:  54%|#####4    | 298/549 [01:31<01:11,  3.53it/s]
2022-07-11 21:25:35,430 - INFO - tqdm - accuracy: 0.9249, batch_loss: 0.3248, loss: 0.2004 ||:  60%|#####9    | 328/549 [01:41<01:16,  2.88it/s]
2022-07-11 21:25:45,668 - INFO - tqdm - accuracy: 0.9242, batch_loss: 0.1761, loss: 0.2010 ||:  66%|######5   | 361/549 [01:51<00:59,  3.17it/s]
2022-07-11 21:25:55,744 - INFO - tqdm - accuracy: 0.9239, batch_loss: 0.0297, loss: 0.1998 ||:  72%|#######1  | 394/549 [02:01<00:33,  4.68it/s]
2022-07-11 21:26:06,041 - INFO - tqdm - accuracy: 0.9242, batch_loss: 0.2223, loss: 0.1993 ||:  78%|#######8  | 429/549 [02:11<00:35,  3.36it/s]
2022-07-11 21:26:16,286 - INFO - tqdm - accuracy: 0.9242, batch_loss: 0.1962, loss: 0.1995 ||:  84%|########3 | 460/549 [02:22<00:26,  3.39it/s]
2022-07-11 21:26:26,361 - INFO - tqdm - accuracy: 0.9262, batch_loss: 0.1084, loss: 0.1943 ||:  90%|######### | 496/549 [02:32<00:11,  4.49it/s]
2022-07-11 21:26:36,426 - INFO - tqdm - accuracy: 0.9235, batch_loss: 0.2242, loss: 0.1987 ||:  97%|#########6| 531/549 [02:42<00:05,  3.04it/s]
2022-07-11 21:26:41,466 - INFO - tqdm - accuracy: 0.9228, batch_loss: 0.0547, loss: 0.2006 ||: 100%|#########9| 547/549 [02:47<00:00,  2.87it/s]
2022-07-11 21:26:41,682 - INFO - tqdm - accuracy: 0.9229, batch_loss: 0.0746, loss: 0.2004 ||: 100%|#########9| 548/549 [02:47<00:00,  3.24it/s]
2022-07-11 21:26:41,752 - INFO - tqdm - accuracy: 0.9229, batch_loss: 0.0481, loss: 0.2001 ||: 100%|##########| 549/549 [02:47<00:00,  3.28it/s]
2022-07-11 21:26:41,752 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 21:26:41,754 - INFO - tqdm - 0%|          | 0/197 [00:00<?, ?it/s]
2022-07-11 21:26:51,861 - INFO - tqdm - accuracy: 0.7742, batch_loss: 2.5329, loss: 0.6081 ||:  50%|####9     | 98/197 [00:10<00:09, 10.42it/s]
2022-07-11 21:27:00,374 - INFO - tqdm - accuracy: 0.7709, batch_loss: 0.1143, loss: 0.6233 ||: 100%|##########| 197/197 [00:18<00:00, 11.02it/s]
2022-07-11 21:27:00,374 - INFO - tqdm - accuracy: 0.7709, batch_loss: 0.1143, loss: 0.6233 ||: 100%|##########| 197/197 [00:18<00:00, 10.58it/s]
2022-07-11 21:27:00,375 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 21:27:00,375 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.923  |     0.771
2022-07-11 21:27:00,375 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7820.602  |       N/A
2022-07-11 21:27:00,375 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.200  |     0.623
2022-07-11 21:27:00,375 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5830.316  |       N/A
2022-07-11 21:27:08,783 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:03:14.532698
2022-07-11 21:27:08,784 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:19:17
2022-07-11 21:27:08,784 - INFO - allennlp.training.gradient_descent_trainer - Epoch 4/9
2022-07-11 21:27:08,784 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.7G
2022-07-11 21:27:08,784 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 21:27:08,785 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 21:27:08,786 - INFO - tqdm - 0%|          | 0/549 [00:00<?, ?it/s]
2022-07-11 21:27:18,985 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0873, loss: 0.1002 ||:   5%|5         | 30/549 [00:10<02:32,  3.40it/s]
2022-07-11 21:27:29,002 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.1635, loss: 0.0954 ||:  11%|#1        | 63/549 [00:20<02:13,  3.64it/s]
2022-07-11 21:27:39,329 - INFO - tqdm - accuracy: 0.9716, batch_loss: 0.0065, loss: 0.0966 ||:  18%|#7        | 97/549 [00:30<02:43,  2.76it/s]
2022-07-11 21:27:49,513 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.4711, loss: 0.1103 ||:  24%|##4       | 132/549 [00:40<02:01,  3.42it/s]
2022-07-11 21:27:59,544 - INFO - tqdm - accuracy: 0.9642, batch_loss: 0.0127, loss: 0.1035 ||:  30%|##9       | 164/549 [00:50<02:20,  2.74it/s]
2022-07-11 21:28:09,578 - INFO - tqdm - accuracy: 0.9666, batch_loss: 0.3406, loss: 0.0985 ||:  37%|###6      | 202/549 [01:00<01:22,  4.21it/s]
2022-07-11 21:28:19,659 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.0141, loss: 0.1077 ||:  43%|####2     | 235/549 [01:10<01:27,  3.61it/s]
2022-07-11 21:28:29,829 - INFO - tqdm - accuracy: 0.9649, batch_loss: 0.0084, loss: 0.1023 ||:  49%|####8     | 267/549 [01:21<01:42,  2.75it/s]
2022-07-11 21:28:39,945 - INFO - tqdm - accuracy: 0.9652, batch_loss: 0.0166, loss: 0.1014 ||:  54%|#####4    | 298/549 [01:31<01:00,  4.16it/s]
2022-07-11 21:28:50,056 - INFO - tqdm - accuracy: 0.9657, batch_loss: 0.5612, loss: 0.1025 ||:  60%|#####9    | 328/549 [01:41<01:09,  3.20it/s]
2022-07-11 21:29:00,117 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0801, loss: 0.1028 ||:  66%|######6   | 365/549 [01:51<00:43,  4.24it/s]
2022-07-11 21:29:10,154 - INFO - tqdm - accuracy: 0.9657, batch_loss: 0.4462, loss: 0.1016 ||:  72%|#######2  | 397/549 [02:01<00:45,  3.31it/s]
2022-07-11 21:29:20,288 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.1474, loss: 0.1042 ||:  79%|#######8  | 431/549 [02:11<00:30,  3.81it/s]
2022-07-11 21:29:30,329 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0092, loss: 0.1006 ||:  85%|########5 | 468/549 [02:21<00:28,  2.82it/s]
2022-07-11 21:29:40,605 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.1040, loss: 0.1027 ||:  91%|#########1| 500/549 [02:31<00:18,  2.63it/s]
2022-07-11 21:29:50,652 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.0305, loss: 0.1034 ||:  97%|#########7| 533/549 [02:41<00:04,  3.83it/s]
2022-07-11 21:29:54,502 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.1450, loss: 0.1038 ||: 100%|#########9| 547/549 [02:45<00:00,  4.26it/s]
2022-07-11 21:29:54,698 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.0266, loss: 0.1037 ||: 100%|#########9| 548/549 [02:45<00:00,  4.48it/s]
2022-07-11 21:29:54,760 - INFO - tqdm - accuracy: 0.9647, batch_loss: 0.1359, loss: 0.1038 ||: 100%|##########| 549/549 [02:45<00:00,  3.31it/s]
2022-07-11 21:29:54,761 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 21:29:54,762 - INFO - tqdm - 0%|          | 0/197 [00:00<?, ?it/s]
2022-07-11 21:30:04,865 - INFO - tqdm - accuracy: 0.7729, batch_loss: 0.0242, loss: 0.8159 ||:  55%|#####5    | 109/197 [00:10<00:08, 10.75it/s]
2022-07-11 21:30:12,977 - INFO - tqdm - accuracy: 0.7735, batch_loss: 0.0427, loss: 0.8301 ||: 100%|##########| 197/197 [00:18<00:00,  9.87it/s]
2022-07-11 21:30:12,977 - INFO - tqdm - accuracy: 0.7735, batch_loss: 0.0427, loss: 0.8301 ||: 100%|##########| 197/197 [00:18<00:00, 10.82it/s]
2022-07-11 21:30:12,978 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 21:30:12,978 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.965  |     0.773
2022-07-11 21:30:12,978 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7820.602  |       N/A
2022-07-11 21:30:12,978 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.104  |     0.830
2022-07-11 21:30:12,978 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5830.316  |       N/A
2022-07-11 21:30:21,237 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:03:12.453301
2022-07-11 21:30:21,237 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:16:04
2022-07-11 21:30:21,237 - INFO - allennlp.training.gradient_descent_trainer - Epoch 5/9
2022-07-11 21:30:21,238 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.7G
2022-07-11 21:30:21,238 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 21:30:21,239 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 21:30:21,240 - INFO - tqdm - 0%|          | 0/549 [00:00<?, ?it/s]
2022-07-11 21:30:31,372 - INFO - tqdm - accuracy: 0.9637, batch_loss: 0.0375, loss: 0.1125 ||:   6%|5         | 31/549 [00:10<02:19,  3.72it/s]
2022-07-11 21:30:41,698 - INFO - tqdm - accuracy: 0.9707, batch_loss: 0.0062, loss: 0.0914 ||:  12%|#1        | 64/549 [00:20<02:39,  3.04it/s]
2022-07-11 21:30:52,027 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.2016, loss: 0.0862 ||:  18%|#7        | 97/549 [00:30<02:25,  3.11it/s]
2022-07-11 21:31:02,319 - INFO - tqdm - accuracy: 0.9729, batch_loss: 0.0041, loss: 0.0787 ||:  23%|##3       | 129/549 [00:41<02:26,  2.87it/s]
2022-07-11 21:31:12,692 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.1549, loss: 0.0763 ||:  29%|##9       | 161/549 [00:51<02:11,  2.95it/s]
2022-07-11 21:31:22,725 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0074, loss: 0.0769 ||:  35%|###5      | 193/549 [01:01<01:56,  3.07it/s]
2022-07-11 21:31:32,751 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0812, loss: 0.0722 ||:  41%|####1     | 226/549 [01:11<01:39,  3.24it/s]
2022-07-11 21:31:42,962 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0053, loss: 0.0669 ||:  47%|####6     | 258/549 [01:21<01:44,  2.79it/s]
2022-07-11 21:31:53,242 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0066, loss: 0.0654 ||:  53%|#####3    | 292/549 [01:32<01:08,  3.74it/s]
2022-07-11 21:32:03,408 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0190, loss: 0.0684 ||:  59%|#####9    | 324/549 [01:42<01:10,  3.18it/s]
2022-07-11 21:32:13,445 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.0373, loss: 0.0707 ||:  65%|######5   | 358/549 [01:52<01:00,  3.17it/s]
2022-07-11 21:32:23,461 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0095, loss: 0.0684 ||:  71%|#######1  | 391/549 [02:02<00:46,  3.41it/s]
2022-07-11 21:32:33,706 - INFO - tqdm - accuracy: 0.9759, batch_loss: 0.0040, loss: 0.0695 ||:  78%|#######7  | 426/549 [02:12<00:41,  2.99it/s]
2022-07-11 21:32:43,846 - INFO - tqdm - accuracy: 0.9747, batch_loss: 0.0036, loss: 0.0733 ||:  84%|########3 | 459/549 [02:22<00:24,  3.61it/s]
2022-07-11 21:32:54,102 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0322, loss: 0.0720 ||:  90%|########9 | 493/549 [02:32<00:16,  3.33it/s]
2022-07-11 21:33:04,515 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.0381, loss: 0.0710 ||:  95%|#########5| 524/549 [02:43<00:08,  2.86it/s]
2022-07-11 21:33:10,788 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0046, loss: 0.0692 ||: 100%|#########9| 547/549 [02:49<00:00,  3.13it/s]
2022-07-11 21:33:11,135 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0059, loss: 0.0691 ||: 100%|#########9| 548/549 [02:49<00:00,  3.05it/s]
2022-07-11 21:33:11,264 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0030, loss: 0.0690 ||: 100%|##########| 549/549 [02:50<00:00,  3.73it/s]
2022-07-11 21:33:11,264 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0030, loss: 0.0690 ||: 100%|##########| 549/549 [02:50<00:00,  3.23it/s]
2022-07-11 21:33:11,265 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 21:33:11,266 - INFO - tqdm - 0%|          | 0/197 [00:00<?, ?it/s]
2022-07-11 21:33:21,320 - INFO - tqdm - accuracy: 0.7639, batch_loss: 0.6786, loss: 0.9401 ||:  55%|#####4    | 108/197 [00:10<00:09,  9.41it/s]
2022-07-11 21:33:29,434 - INFO - tqdm - accuracy: 0.7741, batch_loss: 0.8960, loss: 0.9219 ||: 100%|##########| 197/197 [00:18<00:00, 10.84it/s]
2022-07-11 21:33:29,435 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 21:33:29,435 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.976  |     0.774
2022-07-11 21:33:29,435 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7820.602  |       N/A
2022-07-11 21:33:29,435 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.069  |     0.922
2022-07-11 21:33:29,435 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  5830.316  |       N/A
2022-07-11 21:33:37,725 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:03:16.487334
2022-07-11 21:33:37,725 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:12:53
2022-07-11 21:33:37,725 - INFO - allennlp.training.gradient_descent_trainer - Epoch 6/9
2022-07-11 21:33:37,725 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.7G
2022-07-11 21:33:37,726 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 21:33:37,727 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 21:33:37,727 - INFO - tqdm - 0%|          | 0/549 [00:00<?, ?it/s]
2022-07-11 21:33:46,666 - INFO - root - Training interrupted by the user. Attempting to create a model archive using the current best epoch weights.
2022-07-11 21:33:46,667 - INFO - allennlp.models.archival - archiving weights and vocabulary to bert-model-tagged-location/model.tar.gz
