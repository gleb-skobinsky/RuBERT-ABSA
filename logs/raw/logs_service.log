2022-07-11 01:31:40,533 - INFO - allennlp.common.params - random_seed = 13370
2022-07-11 01:31:40,533 - INFO - allennlp.common.params - numpy_seed = 1337
2022-07-11 01:31:40,533 - INFO - allennlp.common.params - pytorch_seed = 133
2022-07-11 01:31:40,556 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu113
2022-07-11 01:31:40,556 - INFO - allennlp.common.params - type = default
2022-07-11 01:31:40,557 - INFO - allennlp.common.params - dataset_reader.type = classification-tsv
2022-07-11 01:31:40,557 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-07-11 01:31:40,557 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-07-11 01:31:40,557 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2022-07-11 01:31:40,557 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2022-07-11 01:31:40,558 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = DeepPavlov/rubert-base-cased
2022-07-11 01:31:40,558 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2022-07-11 01:31:40,558 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None
2022-07-11 01:31:40,558 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2022-07-11 01:31:40,558 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None
2022-07-11 01:31:51,990 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = pretrained_transformer
2022-07-11 01:31:51,990 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.token_min_padding_length = 0
2022-07-11 01:31:51,990 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.model_name = DeepPavlov/rubert-base-cased
2022-07-11 01:31:51,990 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.namespace = tags
2022-07-11 01:31:51,990 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_length = None
2022-07-11 01:31:51,990 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.tokenizer_kwargs = None
2022-07-11 01:31:51,991 - INFO - allennlp.common.params - dataset_reader.max_tokens = 512
2022-07-11 01:31:51,992 - INFO - allennlp.common.params - train_data_path = /content/service_train.tsv
2022-07-11 01:31:51,992 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f4af3b90750>
2022-07-11 01:31:51,992 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-07-11 01:31:51,992 - INFO - allennlp.common.params - validation_dataset_reader = None
2022-07-11 01:31:51,992 - INFO - allennlp.common.params - validation_data_path = /content/service_test.tsv
2022-07-11 01:31:51,992 - INFO - allennlp.common.params - validation_data_loader = None
2022-07-11 01:31:51,993 - INFO - allennlp.common.params - test_data_path = None
2022-07-11 01:31:51,993 - INFO - allennlp.common.params - evaluate_on_test = False
2022-07-11 01:31:51,993 - INFO - allennlp.common.params - batch_weight_key = 
2022-07-11 01:31:51,993 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-07-11 01:31:51,993 - INFO - allennlp.common.params - data_loader.batch_size = 8
2022-07-11 01:31:51,993 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-07-11 01:31:51,993 - INFO - allennlp.common.params - data_loader.shuffle = True
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.quiet = False
2022-07-11 01:31:51,994 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f4afa883b10>
2022-07-11 01:31:51,995 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-07-11 01:32:02,014 - INFO - tqdm - loading instances: 2401it [00:10, 161.83it/s]
2022-07-11 01:32:12,058 - INFO - tqdm - loading instances: 4438it [00:20, 141.40it/s]
2022-07-11 01:32:22,076 - INFO - tqdm - loading instances: 6975it [00:30, 288.11it/s]
2022-07-11 01:32:29,035 - INFO - allennlp.common.params - data_loader.type = multiprocess
2022-07-11 01:32:29,035 - INFO - allennlp.common.params - data_loader.batch_size = 8
2022-07-11 01:32:29,035 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-07-11 01:32:29,035 - INFO - allennlp.common.params - data_loader.shuffle = True
2022-07-11 01:32:29,035 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2022-07-11 01:32:29,035 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-07-11 01:32:29,036 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-07-11 01:32:29,036 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2022-07-11 01:32:29,036 - INFO - allennlp.common.params - data_loader.start_method = fork
2022-07-11 01:32:29,036 - INFO - allennlp.common.params - data_loader.cuda_device = None
2022-07-11 01:32:29,036 - INFO - allennlp.common.params - data_loader.quiet = False
2022-07-11 01:32:29,036 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f4afa883b10>
2022-07-11 01:32:29,036 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2022-07-11 01:32:35,737 - INFO - allennlp.common.params - type = from_instances
2022-07-11 01:32:35,737 - INFO - allennlp.common.params - min_count = None
2022-07-11 01:32:35,737 - INFO - allennlp.common.params - max_vocab_size = None
2022-07-11 01:32:35,738 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-07-11 01:32:35,738 - INFO - allennlp.common.params - pretrained_files = None
2022-07-11 01:32:35,738 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-07-11 01:32:35,738 - INFO - allennlp.common.params - tokens_to_add = None
2022-07-11 01:32:35,738 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-07-11 01:32:35,738 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-07-11 01:32:35,738 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-07-11 01:32:35,738 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-07-11 01:32:35,738 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-07-11 01:32:35,922 - INFO - allennlp.common.params - model.type = simple_classifier
2022-07-11 01:32:35,922 - INFO - allennlp.common.params - model.embedder.type = basic
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.type = pretrained_transformer
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.model_name = DeepPavlov/rubert-base-cased
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.max_length = None
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.sub_module = None
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.train_parameters = True
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.eval_mode = False
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.last_layer_only = True
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.override_weights_file = None
2022-07-11 01:32:35,923 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.override_weights_strip_prefix = None
2022-07-11 01:32:35,924 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.reinit_modules = None
2022-07-11 01:32:35,924 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.load_weights = True
2022-07-11 01:32:35,924 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.gradient_checkpointing = None
2022-07-11 01:32:35,924 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.tokenizer_kwargs = None
2022-07-11 01:32:35,924 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.transformer_kwargs = None
2022-07-11 01:32:43,132 - INFO - allennlp.common.params - model.encoder.type = bert_pooler
2022-07-11 01:32:43,133 - INFO - allennlp.common.params - model.encoder.pretrained_model = DeepPavlov/rubert-base-cased
2022-07-11 01:32:43,133 - INFO - allennlp.common.params - model.encoder.override_weights_file = None
2022-07-11 01:32:43,133 - INFO - allennlp.common.params - model.encoder.override_weights_strip_prefix = None
2022-07-11 01:32:43,133 - INFO - allennlp.common.params - model.encoder.load_weights = True
2022-07-11 01:32:43,133 - INFO - allennlp.common.params - model.encoder.requires_grad = True
2022-07-11 01:32:43,133 - INFO - allennlp.common.params - model.encoder.dropout = 0.0
2022-07-11 01:32:43,133 - INFO - allennlp.common.params - model.encoder.transformer_kwargs = None
2022-07-11 01:32:43,723 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-07-11 01:32:43,724 - INFO - allennlp.common.params - trainer.cuda_device = None
2022-07-11 01:32:43,724 - INFO - allennlp.common.params - trainer.distributed = False
2022-07-11 01:32:43,724 - INFO - allennlp.common.params - trainer.world_size = 1
2022-07-11 01:32:43,724 - INFO - allennlp.common.params - trainer.patience = None
2022-07-11 01:32:43,724 - INFO - allennlp.common.params - trainer.validation_metric = -loss
2022-07-11 01:32:43,725 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-07-11 01:32:43,725 - INFO - allennlp.common.params - trainer.grad_norm = False
2022-07-11 01:32:43,725 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-07-11 01:32:43,725 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2022-07-11 01:32:43,725 - INFO - allennlp.common.params - trainer.use_amp = False
2022-07-11 01:32:43,725 - INFO - allennlp.common.params - trainer.no_grad = None
2022-07-11 01:32:43,725 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2022-07-11 01:32:43,726 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-07-11 01:32:43,726 - INFO - allennlp.common.params - trainer.moving_average = None
2022-07-11 01:32:43,726 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f4af3bce550>
2022-07-11 01:32:43,726 - INFO - allennlp.common.params - trainer.callbacks = None
2022-07-11 01:32:43,726 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2022-07-11 01:32:43,726 - INFO - allennlp.common.params - trainer.run_confidence_checks = True
2022-07-11 01:32:43,726 - INFO - allennlp.common.params - trainer.grad_scaling = True
2022-07-11 01:32:51,919 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-07-11 01:32:51,946 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-07-11 01:32:51,950 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-07-11 01:32:51,950 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)
2022-07-11 01:32:51,950 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08
2022-07-11 01:32:51,950 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0
2022-07-11 01:32:51,950 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-07-11 01:32:51,950 - INFO - allennlp.training.optimizers - Number of trainable parameters: 178445570
2022-07-11 01:32:52,122 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-07-11 01:32:52,123 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-07-11 01:32:52,123 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.word_embeddings.weight
2022-07-11 01:32:52,123 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.position_embeddings.weight
2022-07-11 01:32:52,123 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.token_type_embeddings.weight
2022-07-11 01:32:52,123 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.LayerNorm.weight
2022-07-11 01:32:52,123 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.embeddings.LayerNorm.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.query.weight
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.query.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.key.weight
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.key.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.value.weight
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.self.value.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.dense.weight
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.dense.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.intermediate.dense.weight
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.intermediate.dense.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.dense.weight
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.dense.bias
2022-07-11 01:32:52,124 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.LayerNorm.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.0.output.LayerNorm.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.query.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.query.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.key.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.key.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.value.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.self.value.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.dense.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.dense.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.intermediate.dense.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.intermediate.dense.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.dense.weight
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.dense.bias
2022-07-11 01:32:52,125 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.LayerNorm.weight
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.1.output.LayerNorm.bias
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.query.weight
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.query.bias
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.key.weight
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.key.bias
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.value.weight
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.self.value.bias
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.dense.weight
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.dense.bias
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.intermediate.dense.weight
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.intermediate.dense.bias
2022-07-11 01:32:52,126 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.dense.weight
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.dense.bias
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.LayerNorm.weight
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.2.output.LayerNorm.bias
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.query.weight
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.query.bias
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.key.weight
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.key.bias
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.value.weight
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.self.value.bias
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.dense.weight
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.dense.bias
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2022-07-11 01:32:52,127 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.intermediate.dense.weight
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.intermediate.dense.bias
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.dense.weight
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.dense.bias
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.LayerNorm.weight
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.3.output.LayerNorm.bias
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.query.weight
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.query.bias
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.key.weight
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.key.bias
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.value.weight
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.self.value.bias
2022-07-11 01:32:52,128 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.dense.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.dense.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.intermediate.dense.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.intermediate.dense.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.dense.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.dense.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.LayerNorm.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.4.output.LayerNorm.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.query.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.query.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.key.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.key.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.value.weight
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.self.value.bias
2022-07-11 01:32:52,129 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.dense.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.dense.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.intermediate.dense.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.intermediate.dense.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.dense.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.dense.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.LayerNorm.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.5.output.LayerNorm.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.query.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.query.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.key.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.key.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.value.weight
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.self.value.bias
2022-07-11 01:32:52,130 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.dense.weight
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.dense.bias
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.intermediate.dense.weight
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.intermediate.dense.bias
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.dense.weight
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.dense.bias
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.LayerNorm.weight
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.6.output.LayerNorm.bias
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.query.weight
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.query.bias
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.key.weight
2022-07-11 01:32:52,131 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.key.bias
2022-07-11 01:32:52,189 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.value.weight
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.self.value.bias
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.dense.weight
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.dense.bias
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.intermediate.dense.weight
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.intermediate.dense.bias
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.dense.weight
2022-07-11 01:32:52,190 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.dense.bias
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.LayerNorm.weight
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.7.output.LayerNorm.bias
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.query.weight
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.query.bias
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.key.weight
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.key.bias
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.value.weight
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.self.value.bias
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.dense.weight
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.dense.bias
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2022-07-11 01:32:52,191 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.intermediate.dense.weight
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.intermediate.dense.bias
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.dense.weight
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.dense.bias
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.LayerNorm.weight
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.8.output.LayerNorm.bias
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.query.weight
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.query.bias
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.key.weight
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.key.bias
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.value.weight
2022-07-11 01:32:52,192 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.self.value.bias
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.dense.weight
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.dense.bias
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.intermediate.dense.weight
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.intermediate.dense.bias
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.dense.weight
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.dense.bias
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.LayerNorm.weight
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.9.output.LayerNorm.bias
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.query.weight
2022-07-11 01:32:52,193 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.query.bias
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.key.weight
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.key.bias
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.value.weight
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.self.value.bias
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.dense.weight
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.dense.bias
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.intermediate.dense.weight
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.intermediate.dense.bias
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.dense.weight
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.dense.bias
2022-07-11 01:32:52,194 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.LayerNorm.weight
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.10.output.LayerNorm.bias
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.query.weight
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.query.bias
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.key.weight
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.key.bias
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.value.weight
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.self.value.bias
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.dense.weight
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.dense.bias
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.intermediate.dense.weight
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.intermediate.dense.bias
2022-07-11 01:32:52,195 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.dense.weight
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.dense.bias
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.LayerNorm.weight
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.encoder.layer.11.output.LayerNorm.bias
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.pooler.dense.weight
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - embedder.token_embedder_bert.transformer_model.pooler.dense.bias
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - encoder.pooler.dense.weight
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - encoder.pooler.dense.bias
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - classifier.weight
2022-07-11 01:32:52,196 - INFO - allennlp.common.util - classifier.bias
2022-07-11 01:32:52,196 - INFO - allennlp.common.params - type = default
2022-07-11 01:32:52,197 - INFO - allennlp.common.params - save_completed_epochs = True
2022-07-11 01:32:52,197 - INFO - allennlp.common.params - save_every_num_seconds = None
2022-07-11 01:32:52,197 - INFO - allennlp.common.params - save_every_num_batches = None
2022-07-11 01:32:52,197 - INFO - allennlp.common.params - keep_most_recent_by_count = 2
2022-07-11 01:32:52,197 - INFO - allennlp.common.params - keep_most_recent_by_age = None
2022-07-11 01:32:52,197 - WARNING - allennlp.training.gradient_descent_trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-07-11 01:32:52,208 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.
2022-07-11 01:32:52,208 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/9
2022-07-11 01:32:52,210 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 01:32:52,224 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 681M
2022-07-11 01:32:52,225 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 01:32:52,229 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 01:32:52,957 - INFO - allennlp.training.callbacks.console_logger - Batch inputs
2022-07-11 01:32:52,957 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/token_ids (Shape: 8 x 512)
tensor([[   101, 118619,  15455,  ...,      0,      0,      0],
        [   101,  79587,   6622,  ...,      0,      0,      0],
        [   101,  67082,  11921,  ...,      0,      0,      0],
        ...,
        [   101, 107081,   2343,  ...,  25736,    852,  46000],
        [   101,  42857,  10917,  ...,      0,      0,      0],
        [   101,  67082,  38853,  ...,      0,      0,      0]],
       device='cuda:0')
2022-07-11 01:32:52,965 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/mask (Shape: 8 x 512)
tensor([[ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')
2022-07-11 01:32:52,967 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/type_ids (Shape: 8 x 512)
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')
2022-07-11 01:32:52,969 - INFO - allennlp.training.callbacks.console_logger - batch_input/label (Shape: 8)
tensor([0, 0, 1,  ..., 0, 1, 1], device='cuda:0')
2022-07-11 01:33:02,246 - INFO - tqdm - accuracy: 0.5776, batch_loss: 0.7477, loss: 0.6758 ||:   3%|2         | 29/1131 [00:10<06:25,  2.86it/s]
2022-07-11 01:33:12,369 - INFO - tqdm - accuracy: 0.6045, batch_loss: 0.6416, loss: 0.6603 ||:   5%|5         | 61/1131 [00:20<05:00,  3.56it/s]
2022-07-11 01:33:22,719 - INFO - tqdm - accuracy: 0.6549, batch_loss: 0.2479, loss: 0.6149 ||:   8%|8         | 92/1131 [00:30<05:55,  2.92it/s]
2022-07-11 01:33:32,999 - INFO - tqdm - accuracy: 0.6865, batch_loss: 0.6590, loss: 0.5878 ||:  11%|#         | 124/1131 [00:40<05:08,  3.26it/s]
2022-07-11 01:33:43,388 - INFO - tqdm - accuracy: 0.7051, batch_loss: 0.2431, loss: 0.5608 ||:  14%|#3        | 156/1131 [00:51<06:13,  2.61it/s]
2022-07-11 01:33:53,714 - INFO - tqdm - accuracy: 0.7182, batch_loss: 0.2713, loss: 0.5399 ||:  16%|#6        | 185/1131 [01:01<05:48,  2.71it/s]
2022-07-11 01:34:03,927 - INFO - tqdm - accuracy: 0.7339, batch_loss: 0.2299, loss: 0.5227 ||:  19%|#9        | 217/1131 [01:11<05:33,  2.74it/s]
2022-07-11 01:34:14,190 - INFO - tqdm - accuracy: 0.7388, batch_loss: 0.6658, loss: 0.5196 ||:  22%|##1       | 245/1131 [01:21<05:12,  2.84it/s]
2022-07-11 01:34:24,479 - INFO - tqdm - accuracy: 0.7446, batch_loss: 0.4927, loss: 0.5133 ||:  25%|##4       | 278/1131 [01:32<04:25,  3.21it/s]
2022-07-11 01:34:34,708 - INFO - tqdm - accuracy: 0.7480, batch_loss: 0.3714, loss: 0.5104 ||:  27%|##7       | 308/1131 [01:42<04:52,  2.81it/s]
2022-07-11 01:34:44,968 - INFO - tqdm - accuracy: 0.7518, batch_loss: 0.3054, loss: 0.5053 ||:  30%|##9       | 338/1131 [01:52<05:11,  2.54it/s]
2022-07-11 01:34:55,294 - INFO - tqdm - accuracy: 0.7537, batch_loss: 0.3139, loss: 0.5092 ||:  33%|###2      | 368/1131 [02:03<04:12,  3.02it/s]
2022-07-11 01:35:05,436 - INFO - tqdm - accuracy: 0.7566, batch_loss: 0.8155, loss: 0.5037 ||:  35%|###5      | 400/1131 [02:13<03:33,  3.42it/s]
2022-07-11 01:35:15,504 - INFO - tqdm - accuracy: 0.7618, batch_loss: 0.2644, loss: 0.4949 ||:  38%|###8      | 434/1131 [02:23<03:05,  3.75it/s]
2022-07-11 01:35:25,531 - INFO - tqdm - accuracy: 0.7667, batch_loss: 0.5065, loss: 0.4881 ||:  41%|####1     | 465/1131 [02:33<03:41,  3.01it/s]
2022-07-11 01:35:35,615 - INFO - tqdm - accuracy: 0.7697, batch_loss: 0.3329, loss: 0.4870 ||:  44%|####3     | 496/1131 [02:43<03:08,  3.37it/s]
2022-07-11 01:35:45,631 - INFO - tqdm - accuracy: 0.7728, batch_loss: 0.3604, loss: 0.4827 ||:  47%|####6     | 526/1131 [02:53<02:50,  3.55it/s]
2022-07-11 01:35:55,705 - INFO - tqdm - accuracy: 0.7720, batch_loss: 0.4993, loss: 0.4819 ||:  49%|####9     | 556/1131 [03:03<02:49,  3.39it/s]
2022-07-11 01:36:05,709 - INFO - tqdm - accuracy: 0.7745, batch_loss: 0.6763, loss: 0.4781 ||:  52%|#####1    | 586/1131 [03:13<03:17,  2.76it/s]
2022-07-11 01:36:15,778 - INFO - tqdm - accuracy: 0.7789, batch_loss: 0.1150, loss: 0.4710 ||:  54%|#####4    | 615/1131 [03:23<02:59,  2.88it/s]
2022-07-11 01:36:26,089 - INFO - tqdm - accuracy: 0.7798, batch_loss: 0.4448, loss: 0.4704 ||:  57%|#####7    | 646/1131 [03:33<02:38,  3.05it/s]
2022-07-11 01:36:36,296 - INFO - tqdm - accuracy: 0.7825, batch_loss: 0.2399, loss: 0.4666 ||:  60%|#####9    | 677/1131 [03:44<02:34,  2.93it/s]
2022-07-11 01:36:46,506 - INFO - tqdm - accuracy: 0.7828, batch_loss: 0.6656, loss: 0.4665 ||:  63%|######2   | 708/1131 [03:54<01:57,  3.60it/s]
2022-07-11 01:36:56,756 - INFO - tqdm - accuracy: 0.7819, batch_loss: 0.3807, loss: 0.4663 ||:  65%|######5   | 740/1131 [04:04<02:21,  2.77it/s]
2022-07-11 01:37:06,852 - INFO - tqdm - accuracy: 0.7822, batch_loss: 0.5260, loss: 0.4643 ||:  68%|######8   | 773/1131 [04:14<01:43,  3.46it/s]
2022-07-11 01:37:16,968 - INFO - tqdm - accuracy: 0.7839, batch_loss: 0.4994, loss: 0.4613 ||:  71%|#######1  | 804/1131 [04:24<02:03,  2.65it/s]
2022-07-11 01:37:27,144 - INFO - tqdm - accuracy: 0.7852, batch_loss: 0.1663, loss: 0.4586 ||:  74%|#######3  | 834/1131 [04:34<01:41,  2.94it/s]
2022-07-11 01:37:37,178 - INFO - tqdm - accuracy: 0.7860, batch_loss: 0.2697, loss: 0.4570 ||:  76%|#######6  | 864/1131 [04:44<01:24,  3.15it/s]
2022-07-11 01:37:47,341 - INFO - tqdm - accuracy: 0.7873, batch_loss: 0.2483, loss: 0.4548 ||:  79%|#######9  | 894/1131 [04:55<01:10,  3.37it/s]
2022-07-11 01:37:57,525 - INFO - tqdm - accuracy: 0.7875, batch_loss: 0.7797, loss: 0.4549 ||:  82%|########2 | 930/1131 [05:05<00:44,  4.52it/s]
2022-07-11 01:38:07,915 - INFO - tqdm - accuracy: 0.7897, batch_loss: 0.3209, loss: 0.4516 ||:  85%|########5 | 963/1131 [05:15<01:02,  2.69it/s]
2022-07-11 01:38:17,949 - INFO - tqdm - accuracy: 0.7912, batch_loss: 0.3682, loss: 0.4491 ||:  88%|########8 | 996/1131 [05:25<00:36,  3.73it/s]
2022-07-11 01:38:28,225 - INFO - tqdm - accuracy: 0.7923, batch_loss: 0.4345, loss: 0.4484 ||:  91%|######### | 1026/1131 [05:35<00:36,  2.87it/s]
2022-07-11 01:38:38,302 - INFO - tqdm - accuracy: 0.7934, batch_loss: 0.2728, loss: 0.4456 ||:  93%|#########3| 1054/1131 [05:46<00:23,  3.29it/s]
2022-07-11 01:38:48,389 - INFO - tqdm - accuracy: 0.7932, batch_loss: 0.8827, loss: 0.4463 ||:  96%|#########5| 1082/1131 [05:56<00:17,  2.73it/s]
2022-07-11 01:38:58,432 - INFO - tqdm - accuracy: 0.7940, batch_loss: 0.4001, loss: 0.4444 ||:  98%|#########8| 1114/1131 [06:06<00:05,  3.04it/s]
2022-07-11 01:39:02,163 - INFO - tqdm - accuracy: 0.7940, batch_loss: 0.1954, loss: 0.4441 ||: 100%|#########9| 1126/1131 [06:09<00:01,  2.86it/s]
2022-07-11 01:39:02,334 - INFO - tqdm - accuracy: 0.7940, batch_loss: 0.4099, loss: 0.4440 ||: 100%|#########9| 1127/1131 [06:10<00:01,  3.37it/s]
2022-07-11 01:39:02,775 - INFO - tqdm - accuracy: 0.7939, batch_loss: 0.8740, loss: 0.4444 ||: 100%|#########9| 1128/1131 [06:10<00:01,  2.94it/s]
2022-07-11 01:39:03,096 - INFO - tqdm - accuracy: 0.7940, batch_loss: 0.2558, loss: 0.4443 ||: 100%|#########9| 1129/1131 [06:10<00:00,  2.99it/s]
2022-07-11 01:39:03,382 - INFO - tqdm - accuracy: 0.7940, batch_loss: 0.2381, loss: 0.4441 ||: 100%|#########9| 1130/1131 [06:11<00:00,  3.13it/s]
2022-07-11 01:39:03,465 - INFO - tqdm - accuracy: 0.7940, batch_loss: 0.6129, loss: 0.4442 ||: 100%|##########| 1131/1131 [06:11<00:00,  3.05it/s]
2022-07-11 01:39:03,466 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 01:39:03,467 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 01:39:03,533 - INFO - allennlp.training.callbacks.console_logger - Batch inputs
2022-07-11 01:39:03,533 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/token_ids (Shape: 8 x 229)
tensor([[  101,  4665, 45017,  ...,     0,     0,     0],
        [  101,  9257, 38891,  ...,     0,     0,     0],
        [  101,   132,   132,  ...,     0,     0,     0],
        ...,
        [  101, 42857, 10917,  ...,     0,     0,     0],
        [  101,   108, 97901,  ...,   132,   108,   102],
        [  101, 67082,  7805,  ...,     0,     0,     0]], device='cuda:0')
2022-07-11 01:39:03,535 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/mask (Shape: 8 x 229)
tensor([[ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        ...,
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')
2022-07-11 01:39:03,538 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/bert/type_ids (Shape: 8 x 229)
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')
2022-07-11 01:39:03,540 - INFO - allennlp.training.callbacks.console_logger - batch_input/label (Shape: 8)
tensor([0, 1, 1,  ..., 1, 0, 1], device='cuda:0')
2022-07-11 01:39:13,522 - INFO - tqdm - accuracy: 0.8080, batch_loss: 0.6292, loss: 0.4200 ||:  38%|###7      | 97/256 [00:10<00:17,  9.16it/s]
2022-07-11 01:39:23,535 - INFO - tqdm - accuracy: 0.8217, batch_loss: 0.2633, loss: 0.3876 ||:  80%|#######9  | 204/256 [00:20<00:05,  9.50it/s]
2022-07-11 01:39:27,895 - INFO - tqdm - accuracy: 0.8240, batch_loss: 0.3833, loss: 0.3862 ||: 100%|#########9| 255/256 [00:24<00:00, 13.53it/s]
2022-07-11 01:39:28,034 - INFO - tqdm - accuracy: 0.8237, batch_loss: 0.6331, loss: 0.3871 ||: 100%|##########| 256/256 [00:24<00:00, 10.42it/s]
2022-07-11 01:39:28,035 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 01:39:28,035 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.794  |     0.824
2022-07-11 01:39:28,036 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |   680.989  |       N/A
2022-07-11 01:39:28,036 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.444  |     0.387
2022-07-11 01:39:28,036 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 01:39:37,785 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:45.576979
2022-07-11 01:39:37,796 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:59:22
2022-07-11 01:39:37,796 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/9
2022-07-11 01:39:37,800 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 01:39:37,801 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 01:39:37,801 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 01:39:37,810 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 01:39:47,869 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.8086, loss: 0.2981 ||:   3%|2         | 32/1131 [00:10<06:07,  2.99it/s]
2022-07-11 01:39:58,296 - INFO - tqdm - accuracy: 0.8574, batch_loss: 0.8370, loss: 0.3257 ||:   6%|5         | 64/1131 [00:20<06:00,  2.96it/s]
2022-07-11 01:40:08,393 - INFO - tqdm - accuracy: 0.8685, batch_loss: 0.2675, loss: 0.3096 ||:   8%|8         | 96/1131 [00:30<06:44,  2.56it/s]
2022-07-11 01:40:18,560 - INFO - tqdm - accuracy: 0.8740, batch_loss: 0.1886, loss: 0.3048 ||:  11%|#1        | 128/1131 [00:40<05:28,  3.05it/s]
2022-07-11 01:40:28,734 - INFO - tqdm - accuracy: 0.8694, batch_loss: 0.3159, loss: 0.3111 ||:  14%|#3        | 157/1131 [00:50<06:13,  2.61it/s]
2022-07-11 01:40:39,102 - INFO - tqdm - accuracy: 0.8644, batch_loss: 0.1621, loss: 0.3165 ||:  17%|#6        | 189/1131 [01:01<06:33,  2.40it/s]
2022-07-11 01:40:49,197 - INFO - tqdm - accuracy: 0.8647, batch_loss: 0.1190, loss: 0.3112 ||:  19%|#9        | 218/1131 [01:11<05:44,  2.65it/s]
2022-07-11 01:40:59,551 - INFO - tqdm - accuracy: 0.8654, batch_loss: 0.4054, loss: 0.3198 ||:  22%|##1       | 248/1131 [01:21<05:11,  2.84it/s]
2022-07-11 01:41:09,987 - INFO - tqdm - accuracy: 0.8652, batch_loss: 0.3781, loss: 0.3232 ||:  25%|##4       | 281/1131 [01:32<04:43,  3.00it/s]
2022-07-11 01:41:20,058 - INFO - tqdm - accuracy: 0.8681, batch_loss: 0.2727, loss: 0.3208 ||:  27%|##7       | 309/1131 [01:42<05:17,  2.59it/s]
2022-07-11 01:41:30,391 - INFO - tqdm - accuracy: 0.8673, batch_loss: 0.3394, loss: 0.3187 ||:  30%|###       | 343/1131 [01:52<03:58,  3.31it/s]
2022-07-11 01:41:40,445 - INFO - tqdm - accuracy: 0.8690, batch_loss: 0.1010, loss: 0.3170 ||:  33%|###2      | 372/1131 [02:02<03:55,  3.23it/s]
2022-07-11 01:41:50,800 - INFO - tqdm - accuracy: 0.8707, batch_loss: 0.1956, loss: 0.3145 ||:  36%|###5      | 406/1131 [02:12<04:39,  2.59it/s]
2022-07-11 01:42:00,905 - INFO - tqdm - accuracy: 0.8713, batch_loss: 0.2097, loss: 0.3144 ||:  39%|###8      | 436/1131 [02:23<03:14,  3.58it/s]
2022-07-11 01:42:11,076 - INFO - tqdm - accuracy: 0.8696, batch_loss: 0.4714, loss: 0.3168 ||:  41%|####1     | 464/1131 [02:33<04:26,  2.50it/s]
2022-07-11 01:42:21,255 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.2589, loss: 0.3196 ||:  44%|####3     | 496/1131 [02:43<03:33,  2.98it/s]
2022-07-11 01:42:31,621 - INFO - tqdm - accuracy: 0.8683, batch_loss: 0.4585, loss: 0.3178 ||:  47%|####6     | 526/1131 [02:53<03:48,  2.65it/s]
2022-07-11 01:42:41,789 - INFO - tqdm - accuracy: 0.8703, batch_loss: 0.0448, loss: 0.3163 ||:  49%|####9     | 558/1131 [03:03<02:41,  3.54it/s]
2022-07-11 01:42:51,938 - INFO - tqdm - accuracy: 0.8688, batch_loss: 0.6750, loss: 0.3180 ||:  52%|#####1    | 585/1131 [03:14<03:31,  2.58it/s]
2022-07-11 01:43:01,984 - INFO - tqdm - accuracy: 0.8668, batch_loss: 0.2749, loss: 0.3219 ||:  54%|#####4    | 613/1131 [03:24<02:44,  3.16it/s]
2022-07-11 01:43:12,297 - INFO - tqdm - accuracy: 0.8667, batch_loss: 0.3253, loss: 0.3210 ||:  57%|#####7    | 646/1131 [03:34<02:36,  3.10it/s]
2022-07-11 01:43:22,617 - INFO - tqdm - accuracy: 0.8654, batch_loss: 0.7622, loss: 0.3228 ||:  60%|#####9    | 678/1131 [03:44<02:36,  2.89it/s]
2022-07-11 01:43:32,800 - INFO - tqdm - accuracy: 0.8635, batch_loss: 0.4624, loss: 0.3250 ||:  63%|######2   | 708/1131 [03:54<02:22,  2.96it/s]
2022-07-11 01:43:43,079 - INFO - tqdm - accuracy: 0.8621, batch_loss: 0.0337, loss: 0.3276 ||:  65%|######5   | 737/1131 [04:05<02:43,  2.41it/s]
2022-07-11 01:43:53,182 - INFO - tqdm - accuracy: 0.8608, batch_loss: 0.4994, loss: 0.3288 ||:  68%|######7   | 768/1131 [04:15<01:52,  3.22it/s]
2022-07-11 01:44:03,249 - INFO - tqdm - accuracy: 0.8611, batch_loss: 0.7111, loss: 0.3279 ||:  71%|#######   | 798/1131 [04:25<01:55,  2.88it/s]
2022-07-11 01:44:13,537 - INFO - tqdm - accuracy: 0.8592, batch_loss: 0.8395, loss: 0.3298 ||:  73%|#######3  | 831/1131 [04:35<01:37,  3.09it/s]
2022-07-11 01:44:23,966 - INFO - tqdm - accuracy: 0.8594, batch_loss: 0.0761, loss: 0.3294 ||:  76%|#######6  | 864/1131 [04:46<01:23,  3.20it/s]
2022-07-11 01:44:34,285 - INFO - tqdm - accuracy: 0.8601, batch_loss: 0.5377, loss: 0.3286 ||:  79%|#######9  | 898/1131 [04:56<01:13,  3.19it/s]
2022-07-11 01:44:44,698 - INFO - tqdm - accuracy: 0.8606, batch_loss: 0.2332, loss: 0.3286 ||:  82%|########2 | 931/1131 [05:06<00:58,  3.43it/s]
2022-07-11 01:44:54,805 - INFO - tqdm - accuracy: 0.8602, batch_loss: 0.3566, loss: 0.3284 ||:  85%|########4 | 961/1131 [05:16<01:08,  2.48it/s]
2022-07-11 01:45:04,965 - INFO - tqdm - accuracy: 0.8595, batch_loss: 0.3207, loss: 0.3286 ||:  88%|########7 | 992/1131 [05:27<00:49,  2.79it/s]
2022-07-11 01:45:15,011 - INFO - tqdm - accuracy: 0.8595, batch_loss: 0.3337, loss: 0.3294 ||:  90%|######### | 1023/1131 [05:37<00:37,  2.89it/s]
2022-07-11 01:45:25,096 - INFO - tqdm - accuracy: 0.8603, batch_loss: 0.6275, loss: 0.3270 ||:  93%|#########3| 1054/1131 [05:47<00:27,  2.78it/s]
2022-07-11 01:45:35,417 - INFO - tqdm - accuracy: 0.8607, batch_loss: 0.3220, loss: 0.3266 ||:  96%|#########5| 1084/1131 [05:57<00:15,  3.02it/s]
2022-07-11 01:45:45,508 - INFO - tqdm - accuracy: 0.8604, batch_loss: 0.1188, loss: 0.3269 ||:  99%|#########8| 1115/1131 [06:07<00:04,  3.68it/s]
2022-07-11 01:45:49,113 - INFO - tqdm - accuracy: 0.8610, batch_loss: 0.1133, loss: 0.3255 ||: 100%|#########9| 1126/1131 [06:11<00:01,  3.20it/s]
2022-07-11 01:45:49,554 - INFO - tqdm - accuracy: 0.8610, batch_loss: 0.2191, loss: 0.3254 ||: 100%|#########9| 1127/1131 [06:11<00:01,  2.85it/s]
2022-07-11 01:45:49,914 - INFO - tqdm - accuracy: 0.8608, batch_loss: 0.9290, loss: 0.3260 ||: 100%|#########9| 1128/1131 [06:12<00:01,  2.83it/s]
2022-07-11 01:45:50,159 - INFO - tqdm - accuracy: 0.8609, batch_loss: 0.0418, loss: 0.3257 ||: 100%|#########9| 1129/1131 [06:12<00:00,  3.11it/s]
2022-07-11 01:45:50,601 - INFO - tqdm - accuracy: 0.8610, batch_loss: 0.2222, loss: 0.3256 ||: 100%|#########9| 1130/1131 [06:12<00:00,  2.80it/s]
2022-07-11 01:45:50,662 - INFO - tqdm - accuracy: 0.8609, batch_loss: 0.4393, loss: 0.3257 ||: 100%|##########| 1131/1131 [06:12<00:00,  3.03it/s]
2022-07-11 01:45:50,663 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 01:45:50,664 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 01:46:00,775 - INFO - tqdm - accuracy: 0.8456, batch_loss: 0.0166, loss: 0.4092 ||:  40%|###9      | 102/256 [00:10<00:16,  9.58it/s]
2022-07-11 01:46:10,876 - INFO - tqdm - accuracy: 0.8450, batch_loss: 0.0559, loss: 0.4120 ||:  80%|#######9  | 204/256 [00:20<00:04, 11.56it/s]
2022-07-11 01:46:15,923 - INFO - tqdm - accuracy: 0.8413, batch_loss: 1.3380, loss: 0.4157 ||: 100%|##########| 256/256 [00:25<00:00,  9.77it/s]
2022-07-11 01:46:15,924 - INFO - tqdm - accuracy: 0.8413, batch_loss: 1.3380, loss: 0.4157 ||: 100%|##########| 256/256 [00:25<00:00, 10.13it/s]
2022-07-11 01:46:15,924 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 01:46:15,924 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.861  |     0.841
2022-07-11 01:46:15,924 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 01:46:15,924 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.326  |     0.416
2022-07-11 01:46:15,925 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 01:46:25,518 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:47.721672
2022-07-11 01:46:25,522 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:53:34
2022-07-11 01:46:25,522 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/9
2022-07-11 01:46:25,523 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 01:46:25,523 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 01:46:25,524 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 01:46:25,531 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 01:46:35,738 - INFO - tqdm - accuracy: 0.9292, batch_loss: 0.0496, loss: 0.2174 ||:   3%|2         | 30/1131 [00:10<06:58,  2.63it/s]
2022-07-11 01:46:45,959 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.3897, loss: 0.2192 ||:   5%|5         | 60/1131 [00:20<06:46,  2.63it/s]
2022-07-11 01:46:56,320 - INFO - tqdm - accuracy: 0.9157, batch_loss: 0.1750, loss: 0.2155 ||:   8%|7         | 89/1131 [00:30<06:53,  2.52it/s]
2022-07-11 01:47:06,658 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.0303, loss: 0.2179 ||:  11%|#         | 120/1131 [00:41<06:33,  2.57it/s]
2022-07-11 01:47:16,681 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.2040, loss: 0.2183 ||:  13%|#3        | 150/1131 [00:51<05:09,  3.17it/s]
2022-07-11 01:47:26,906 - INFO - tqdm - accuracy: 0.9160, batch_loss: 0.1074, loss: 0.2135 ||:  16%|#5        | 180/1131 [01:01<04:49,  3.28it/s]
2022-07-11 01:47:37,012 - INFO - tqdm - accuracy: 0.9203, batch_loss: 0.0912, loss: 0.2063 ||:  18%|#8        | 207/1131 [01:11<06:05,  2.53it/s]
2022-07-11 01:47:47,348 - INFO - tqdm - accuracy: 0.9191, batch_loss: 0.0735, loss: 0.2070 ||:  21%|##1       | 238/1131 [01:21<04:27,  3.34it/s]
2022-07-11 01:47:57,365 - INFO - tqdm - accuracy: 0.9176, batch_loss: 0.2085, loss: 0.2081 ||:  24%|##3       | 270/1131 [01:31<04:20,  3.31it/s]
2022-07-11 01:48:07,551 - INFO - tqdm - accuracy: 0.9183, batch_loss: 0.0804, loss: 0.2068 ||:  27%|##6       | 300/1131 [01:42<04:56,  2.80it/s]
2022-07-11 01:48:17,718 - INFO - tqdm - accuracy: 0.9169, batch_loss: 0.0806, loss: 0.2121 ||:  29%|##9       | 331/1131 [01:52<04:51,  2.75it/s]
2022-07-11 01:48:28,024 - INFO - tqdm - accuracy: 0.9177, batch_loss: 0.0895, loss: 0.2098 ||:  32%|###1      | 360/1131 [02:02<05:26,  2.36it/s]
2022-07-11 01:48:38,144 - INFO - tqdm - accuracy: 0.9161, batch_loss: 0.1913, loss: 0.2135 ||:  34%|###4      | 389/1131 [02:12<05:12,  2.37it/s]
2022-07-11 01:48:48,350 - INFO - tqdm - accuracy: 0.9162, batch_loss: 0.1741, loss: 0.2133 ||:  37%|###7      | 419/1131 [02:22<04:28,  2.65it/s]
2022-07-11 01:48:58,625 - INFO - tqdm - accuracy: 0.9162, batch_loss: 0.5535, loss: 0.2157 ||:  40%|###9      | 452/1131 [02:33<03:27,  3.27it/s]
2022-07-11 01:49:08,775 - INFO - tqdm - accuracy: 0.9162, batch_loss: 0.0988, loss: 0.2157 ||:  43%|####2     | 482/1131 [02:43<03:10,  3.41it/s]
2022-07-11 01:49:18,860 - INFO - tqdm - accuracy: 0.9170, batch_loss: 0.0516, loss: 0.2134 ||:  45%|####5     | 512/1131 [02:53<03:49,  2.70it/s]
2022-07-11 01:49:29,211 - INFO - tqdm - accuracy: 0.9144, batch_loss: 0.1420, loss: 0.2182 ||:  48%|####7     | 542/1131 [03:03<03:15,  3.02it/s]
2022-07-11 01:49:39,305 - INFO - tqdm - accuracy: 0.9148, batch_loss: 0.0733, loss: 0.2180 ||:  51%|#####     | 575/1131 [03:13<02:48,  3.30it/s]
2022-07-11 01:49:49,704 - INFO - tqdm - accuracy: 0.9137, batch_loss: 0.2188, loss: 0.2195 ||:  54%|#####3    | 607/1131 [03:24<03:06,  2.81it/s]
2022-07-11 01:49:59,955 - INFO - tqdm - accuracy: 0.9141, batch_loss: 0.3120, loss: 0.2196 ||:  56%|#####6    | 639/1131 [03:34<02:41,  3.05it/s]
2022-07-11 01:50:10,076 - INFO - tqdm - accuracy: 0.9136, batch_loss: 0.0585, loss: 0.2207 ||:  59%|#####9    | 671/1131 [03:44<02:06,  3.64it/s]
2022-07-11 01:50:20,412 - INFO - tqdm - accuracy: 0.9139, batch_loss: 0.4390, loss: 0.2206 ||:  62%|######1   | 701/1131 [03:54<02:43,  2.64it/s]
2022-07-11 01:50:30,483 - INFO - tqdm - accuracy: 0.9132, batch_loss: 0.0747, loss: 0.2213 ||:  65%|######4   | 730/1131 [04:04<02:24,  2.78it/s]
2022-07-11 01:50:40,710 - INFO - tqdm - accuracy: 0.9130, batch_loss: 0.1420, loss: 0.2214 ||:  67%|######7   | 760/1131 [04:15<01:54,  3.24it/s]
2022-07-11 01:50:50,827 - INFO - tqdm - accuracy: 0.9119, batch_loss: 0.4649, loss: 0.2237 ||:  70%|#######   | 792/1131 [04:25<01:48,  3.11it/s]
2022-07-11 01:51:00,982 - INFO - tqdm - accuracy: 0.9109, batch_loss: 0.3351, loss: 0.2257 ||:  73%|#######2  | 822/1131 [04:35<01:29,  3.46it/s]
2022-07-11 01:51:11,312 - INFO - tqdm - accuracy: 0.9106, batch_loss: 0.4548, loss: 0.2262 ||:  76%|#######5  | 854/1131 [04:45<01:40,  2.77it/s]
2022-07-11 01:51:21,479 - INFO - tqdm - accuracy: 0.9109, batch_loss: 0.0977, loss: 0.2246 ||:  78%|#######8  | 887/1131 [04:55<01:06,  3.65it/s]
2022-07-11 01:51:31,628 - INFO - tqdm - accuracy: 0.9103, batch_loss: 0.1848, loss: 0.2265 ||:  81%|########1 | 918/1131 [05:06<01:11,  2.96it/s]
2022-07-11 01:51:41,796 - INFO - tqdm - accuracy: 0.9095, batch_loss: 0.3330, loss: 0.2273 ||:  84%|########3 | 949/1131 [05:16<00:55,  3.29it/s]
2022-07-11 01:51:51,941 - INFO - tqdm - accuracy: 0.9099, batch_loss: 0.1716, loss: 0.2283 ||:  87%|########6 | 981/1131 [05:26<00:48,  3.11it/s]
2022-07-11 01:52:02,126 - INFO - tqdm - accuracy: 0.9097, batch_loss: 0.0587, loss: 0.2285 ||:  89%|########9 | 1011/1131 [05:36<00:32,  3.66it/s]
2022-07-11 01:52:12,179 - INFO - tqdm - accuracy: 0.9103, batch_loss: 0.0126, loss: 0.2264 ||:  92%|#########2| 1044/1131 [05:46<00:26,  3.26it/s]
2022-07-11 01:52:22,406 - INFO - tqdm - accuracy: 0.9099, batch_loss: 0.2532, loss: 0.2274 ||:  95%|#########5| 1076/1131 [05:56<00:20,  2.69it/s]
2022-07-11 01:52:32,421 - INFO - tqdm - accuracy: 0.9099, batch_loss: 0.0530, loss: 0.2285 ||:  98%|#########7| 1108/1131 [06:06<00:07,  3.25it/s]
2022-07-11 01:52:38,213 - INFO - tqdm - accuracy: 0.9101, batch_loss: 0.4594, loss: 0.2282 ||: 100%|#########9| 1126/1131 [06:12<00:01,  2.88it/s]
2022-07-11 01:52:38,505 - INFO - tqdm - accuracy: 0.9102, batch_loss: 0.0403, loss: 0.2281 ||: 100%|#########9| 1127/1131 [06:12<00:01,  3.02it/s]
2022-07-11 01:52:38,944 - INFO - tqdm - accuracy: 0.9102, batch_loss: 0.0755, loss: 0.2279 ||: 100%|#########9| 1128/1131 [06:13<00:01,  2.75it/s]
2022-07-11 01:52:39,387 - INFO - tqdm - accuracy: 0.9102, batch_loss: 0.5806, loss: 0.2282 ||: 100%|#########9| 1129/1131 [06:13<00:00,  2.58it/s]
2022-07-11 01:52:39,632 - INFO - tqdm - accuracy: 0.9101, batch_loss: 0.7119, loss: 0.2287 ||: 100%|#########9| 1130/1131 [06:14<00:00,  2.90it/s]
2022-07-11 01:52:39,687 - INFO - tqdm - accuracy: 0.9101, batch_loss: 0.1600, loss: 0.2286 ||: 100%|##########| 1131/1131 [06:14<00:00,  3.02it/s]
2022-07-11 01:52:39,687 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 01:52:39,689 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 01:52:49,867 - INFO - tqdm - accuracy: 0.8551, batch_loss: 1.4432, loss: 0.4152 ||:  44%|####4     | 113/256 [00:10<00:13, 10.26it/s]
2022-07-11 01:52:59,923 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.6608, loss: 0.4231 ||:  81%|########1 | 208/256 [00:20<00:05,  8.48it/s]
2022-07-11 01:53:04,787 - INFO - tqdm - accuracy: 0.8364, batch_loss: 0.5354, loss: 0.4370 ||: 100%|##########| 256/256 [00:25<00:00, 11.86it/s]
2022-07-11 01:53:04,788 - INFO - tqdm - accuracy: 0.8364, batch_loss: 0.5354, loss: 0.4370 ||: 100%|##########| 256/256 [00:25<00:00, 10.20it/s]
2022-07-11 01:53:04,788 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 01:53:04,788 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.910  |     0.836
2022-07-11 01:53:04,789 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 01:53:04,789 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.229  |     0.437
2022-07-11 01:53:04,789 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 01:53:14,367 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:48.838786
2022-07-11 01:53:14,378 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:47:09
2022-07-11 01:53:14,379 - INFO - allennlp.training.gradient_descent_trainer - Epoch 3/9
2022-07-11 01:53:14,379 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 01:53:14,379 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 01:53:14,380 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 01:53:14,384 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 01:53:24,559 - INFO - tqdm - accuracy: 0.9735, batch_loss: 0.0194, loss: 0.1052 ||:   3%|2         | 33/1131 [00:10<06:24,  2.85it/s]
2022-07-11 01:53:34,880 - INFO - tqdm - accuracy: 0.9640, batch_loss: 0.2621, loss: 0.1096 ||:   6%|5         | 66/1131 [00:20<06:18,  2.81it/s]
2022-07-11 01:53:45,298 - INFO - tqdm - accuracy: 0.9571, batch_loss: 0.2927, loss: 0.1294 ||:   9%|9         | 102/1131 [00:30<06:12,  2.77it/s]
2022-07-11 01:53:55,476 - INFO - tqdm - accuracy: 0.9511, batch_loss: 0.0543, loss: 0.1385 ||:  12%|#1        | 133/1131 [00:41<04:55,  3.37it/s]
2022-07-11 01:54:05,635 - INFO - tqdm - accuracy: 0.9526, batch_loss: 0.0112, loss: 0.1378 ||:  14%|#4        | 161/1131 [00:51<05:59,  2.70it/s]
2022-07-11 01:54:15,769 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.0109, loss: 0.1380 ||:  17%|#6        | 192/1131 [01:01<04:55,  3.17it/s]
2022-07-11 01:54:26,106 - INFO - tqdm - accuracy: 0.9526, batch_loss: 0.0133, loss: 0.1334 ||:  20%|#9        | 224/1131 [01:11<04:52,  3.10it/s]
2022-07-11 01:54:36,503 - INFO - tqdm - accuracy: 0.9520, batch_loss: 0.0986, loss: 0.1327 ||:  23%|##2       | 258/1131 [01:22<04:24,  3.30it/s]
2022-07-11 01:54:46,539 - INFO - tqdm - accuracy: 0.9503, batch_loss: 0.1231, loss: 0.1353 ||:  26%|##5       | 289/1131 [01:32<04:20,  3.23it/s]
2022-07-11 01:54:56,546 - INFO - tqdm - accuracy: 0.9484, batch_loss: 0.3712, loss: 0.1390 ||:  28%|##8       | 322/1131 [01:42<04:14,  3.18it/s]
2022-07-11 01:55:06,818 - INFO - tqdm - accuracy: 0.9495, batch_loss: 0.1144, loss: 0.1400 ||:  31%|###1      | 354/1131 [01:52<05:03,  2.56it/s]
2022-07-11 01:55:16,956 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0145, loss: 0.1402 ||:  34%|###3      | 383/1131 [02:02<05:11,  2.40it/s]
2022-07-11 01:55:26,966 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.1146, loss: 0.1354 ||:  37%|###6      | 413/1131 [02:12<03:39,  3.27it/s]
2022-07-11 01:55:37,089 - INFO - tqdm - accuracy: 0.9489, batch_loss: 0.1651, loss: 0.1441 ||:  39%|###9      | 443/1131 [02:22<04:26,  2.58it/s]
2022-07-11 01:55:47,119 - INFO - tqdm - accuracy: 0.9480, batch_loss: 0.1244, loss: 0.1461 ||:  42%|####1     | 474/1131 [02:32<03:20,  3.27it/s]
2022-07-11 01:55:57,348 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0138, loss: 0.1415 ||:  44%|####4     | 503/1131 [02:42<04:13,  2.48it/s]
2022-07-11 01:56:07,391 - INFO - tqdm - accuracy: 0.9490, batch_loss: 0.0327, loss: 0.1453 ||:  47%|####7     | 534/1131 [02:53<03:31,  2.82it/s]
2022-07-11 01:56:17,536 - INFO - tqdm - accuracy: 0.9495, batch_loss: 0.0765, loss: 0.1429 ||:  50%|####9     | 564/1131 [03:03<03:28,  2.72it/s]
2022-07-11 01:56:27,550 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0073, loss: 0.1412 ||:  53%|#####2    | 596/1131 [03:13<03:04,  2.90it/s]
2022-07-11 01:56:37,670 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0606, loss: 0.1396 ||:  55%|#####5    | 627/1131 [03:23<03:09,  2.66it/s]
2022-07-11 01:56:47,675 - INFO - tqdm - accuracy: 0.9512, batch_loss: 0.0112, loss: 0.1386 ||:  58%|#####8    | 656/1131 [03:33<02:25,  3.26it/s]
2022-07-11 01:56:57,993 - INFO - tqdm - accuracy: 0.9526, batch_loss: 0.0164, loss: 0.1358 ||:  61%|######    | 686/1131 [03:43<02:40,  2.78it/s]
2022-07-11 01:57:08,046 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.0175, loss: 0.1352 ||:  63%|######3   | 715/1131 [03:53<02:49,  2.45it/s]
2022-07-11 01:57:18,217 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0540, loss: 0.1328 ||:  66%|######5   | 746/1131 [04:03<02:24,  2.66it/s]
2022-07-11 01:57:28,263 - INFO - tqdm - accuracy: 0.9520, batch_loss: 0.1543, loss: 0.1356 ||:  69%|######8   | 776/1131 [04:13<01:38,  3.60it/s]
2022-07-11 01:57:38,373 - INFO - tqdm - accuracy: 0.9522, batch_loss: 0.0067, loss: 0.1353 ||:  72%|#######1  | 810/1131 [04:23<01:38,  3.27it/s]
2022-07-11 01:57:48,373 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.6543, loss: 0.1370 ||:  74%|#######4  | 841/1131 [04:33<01:29,  3.24it/s]
2022-07-11 01:57:58,465 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0773, loss: 0.1386 ||:  77%|#######7  | 872/1131 [04:44<01:43,  2.51it/s]
2022-07-11 01:58:08,679 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.0374, loss: 0.1371 ||:  80%|########  | 907/1131 [04:54<01:05,  3.40it/s]
2022-07-11 01:58:18,878 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.0865, loss: 0.1360 ||:  83%|########2 | 936/1131 [05:04<01:01,  3.18it/s]
2022-07-11 01:58:29,112 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.2129, loss: 0.1367 ||:  85%|########5 | 967/1131 [05:14<00:40,  4.10it/s]
2022-07-11 01:58:39,255 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.0506, loss: 0.1387 ||:  88%|########8 | 998/1131 [05:24<00:45,  2.92it/s]
2022-07-11 01:58:49,265 - INFO - tqdm - accuracy: 0.9506, batch_loss: 0.0607, loss: 0.1367 ||:  91%|######### | 1028/1131 [05:34<00:29,  3.52it/s]
2022-07-11 01:58:59,452 - INFO - tqdm - accuracy: 0.9503, batch_loss: 0.0130, loss: 0.1369 ||:  93%|#########3| 1057/1131 [05:45<00:24,  3.07it/s]
2022-07-11 01:59:09,692 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.0365, loss: 0.1368 ||:  96%|#########6| 1090/1131 [05:55<00:13,  3.12it/s]
2022-07-11 01:59:19,713 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.0101, loss: 0.1377 ||:  99%|#########8| 1119/1131 [06:05<00:04,  2.92it/s]
2022-07-11 01:59:21,811 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.0048, loss: 0.1375 ||: 100%|#########9| 1126/1131 [06:07<00:01,  3.15it/s]
2022-07-11 01:59:22,251 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0573, loss: 0.1374 ||: 100%|#########9| 1127/1131 [06:07<00:01,  2.82it/s]
2022-07-11 01:59:22,443 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0048, loss: 0.1373 ||: 100%|#########9| 1128/1131 [06:08<00:00,  3.27it/s]
2022-07-11 01:59:22,885 - INFO - tqdm - accuracy: 0.9497, batch_loss: 0.5618, loss: 0.1377 ||: 100%|#########9| 1129/1131 [06:08<00:00,  2.89it/s]
2022-07-11 01:59:23,326 - INFO - tqdm - accuracy: 0.9497, batch_loss: 0.1798, loss: 0.1377 ||: 100%|#########9| 1130/1131 [06:08<00:00,  2.67it/s]
2022-07-11 01:59:23,423 - INFO - tqdm - accuracy: 0.9497, batch_loss: 0.0044, loss: 0.1376 ||: 100%|##########| 1131/1131 [06:09<00:00,  3.06it/s]
2022-07-11 01:59:23,423 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 01:59:23,425 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 01:59:33,575 - INFO - tqdm - accuracy: 0.8310, batch_loss: 0.3751, loss: 0.5336 ||:  41%|####1     | 105/256 [00:10<00:15,  9.44it/s]
2022-07-11 01:59:43,739 - INFO - tqdm - accuracy: 0.8319, batch_loss: 0.1647, loss: 0.5266 ||:  82%|########1 | 209/256 [00:20<00:05,  9.18it/s]
2022-07-11 01:59:48,086 - INFO - tqdm - accuracy: 0.8230, batch_loss: 0.2608, loss: 0.5502 ||: 100%|#########9| 255/256 [00:24<00:00,  8.79it/s]
2022-07-11 01:59:48,221 - INFO - tqdm - accuracy: 0.8232, batch_loss: 0.1966, loss: 0.5488 ||: 100%|##########| 256/256 [00:24<00:00,  8.47it/s]
2022-07-11 01:59:48,222 - INFO - tqdm - accuracy: 0.8232, batch_loss: 0.1966, loss: 0.5488 ||: 100%|##########| 256/256 [00:24<00:00, 10.32it/s]
2022-07-11 01:59:48,222 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 01:59:48,222 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.950  |     0.823
2022-07-11 01:59:48,222 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 01:59:48,223 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.138  |     0.549
2022-07-11 01:59:48,223 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 01:59:57,780 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:43.401042
2022-07-11 01:59:57,781 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:40:24
2022-07-11 01:59:57,781 - INFO - allennlp.training.gradient_descent_trainer - Epoch 4/9
2022-07-11 01:59:57,781 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 01:59:57,782 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 01:59:57,783 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 01:59:57,785 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 02:00:07,841 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.0131, loss: 0.0543 ||:   3%|2         | 31/1131 [00:10<05:31,  3.32it/s]
2022-07-11 02:00:18,089 - INFO - tqdm - accuracy: 0.9802, batch_loss: 0.0028, loss: 0.0428 ||:   6%|5         | 63/1131 [00:20<06:03,  2.94it/s]
2022-07-11 02:00:28,325 - INFO - tqdm - accuracy: 0.9794, batch_loss: 0.0415, loss: 0.0444 ||:   8%|8         | 91/1131 [00:30<06:46,  2.56it/s]
2022-07-11 02:00:38,510 - INFO - tqdm - accuracy: 0.9737, batch_loss: 0.0305, loss: 0.0583 ||:  11%|#         | 119/1131 [00:40<06:04,  2.77it/s]
2022-07-11 02:00:48,539 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.2291, loss: 0.0665 ||:  13%|#3        | 149/1131 [00:50<04:47,  3.42it/s]
2022-07-11 02:00:58,774 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0060, loss: 0.0665 ||:  16%|#5        | 180/1131 [01:00<05:50,  2.72it/s]
2022-07-11 02:01:08,880 - INFO - tqdm - accuracy: 0.9736, batch_loss: 0.0064, loss: 0.0755 ||:  19%|#8        | 213/1131 [01:11<03:59,  3.83it/s]
2022-07-11 02:01:19,093 - INFO - tqdm - accuracy: 0.9701, batch_loss: 0.0461, loss: 0.0817 ||:  22%|##1       | 247/1131 [01:21<04:17,  3.43it/s]
2022-07-11 02:01:29,267 - INFO - tqdm - accuracy: 0.9711, batch_loss: 0.0416, loss: 0.0796 ||:  24%|##4       | 277/1131 [01:31<05:21,  2.66it/s]
2022-07-11 02:01:39,287 - INFO - tqdm - accuracy: 0.9714, batch_loss: 0.0039, loss: 0.0791 ||:  27%|##7       | 306/1131 [01:41<04:31,  3.04it/s]
2022-07-11 02:01:49,472 - INFO - tqdm - accuracy: 0.9727, batch_loss: 0.0041, loss: 0.0756 ||:  30%|##9       | 334/1131 [01:51<04:04,  3.26it/s]
2022-07-11 02:01:59,684 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0019, loss: 0.0739 ||:  32%|###2      | 364/1131 [02:01<05:00,  2.55it/s]
2022-07-11 02:02:09,942 - INFO - tqdm - accuracy: 0.9726, batch_loss: 0.0081, loss: 0.0773 ||:  35%|###4      | 393/1131 [02:12<05:01,  2.44it/s]
2022-07-11 02:02:20,223 - INFO - tqdm - accuracy: 0.9731, batch_loss: 0.2474, loss: 0.0761 ||:  38%|###7      | 427/1131 [02:22<03:48,  3.08it/s]
2022-07-11 02:02:30,238 - INFO - tqdm - accuracy: 0.9728, batch_loss: 0.0216, loss: 0.0750 ||:  40%|####      | 455/1131 [02:32<03:15,  3.46it/s]
2022-07-11 02:02:40,244 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0358, loss: 0.0777 ||:  43%|####2     | 482/1131 [02:42<03:51,  2.80it/s]
2022-07-11 02:02:50,520 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0989, loss: 0.0772 ||:  45%|####5     | 514/1131 [02:52<03:22,  3.05it/s]
2022-07-11 02:03:00,714 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.3042, loss: 0.0762 ||:  48%|####7     | 541/1131 [03:02<03:41,  2.67it/s]
2022-07-11 02:03:10,940 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.1327, loss: 0.0787 ||:  51%|#####     | 573/1131 [03:13<02:59,  3.11it/s]
2022-07-11 02:03:21,014 - INFO - tqdm - accuracy: 0.9724, batch_loss: 0.2008, loss: 0.0767 ||:  53%|#####3    | 602/1131 [03:23<03:04,  2.87it/s]
2022-07-11 02:03:31,104 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.4573, loss: 0.0783 ||:  56%|#####5    | 633/1131 [03:33<03:16,  2.54it/s]
2022-07-11 02:03:41,385 - INFO - tqdm - accuracy: 0.9726, batch_loss: 0.0076, loss: 0.0774 ||:  59%|#####8    | 662/1131 [03:43<03:07,  2.50it/s]
2022-07-11 02:03:51,453 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0094, loss: 0.0809 ||:  61%|######1   | 692/1131 [03:53<02:22,  3.08it/s]
2022-07-11 02:04:01,589 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.2827, loss: 0.0808 ||:  64%|######4   | 725/1131 [04:03<01:32,  4.41it/s]
2022-07-11 02:04:11,735 - INFO - tqdm - accuracy: 0.9717, batch_loss: 0.0158, loss: 0.0822 ||:  67%|######7   | 761/1131 [04:13<01:54,  3.24it/s]
2022-07-11 02:04:22,126 - INFO - tqdm - accuracy: 0.9713, batch_loss: 0.0284, loss: 0.0826 ||:  70%|#######   | 794/1131 [04:24<01:48,  3.12it/s]
2022-07-11 02:04:32,339 - INFO - tqdm - accuracy: 0.9715, batch_loss: 0.1632, loss: 0.0822 ||:  73%|#######3  | 826/1131 [04:34<01:47,  2.84it/s]
2022-07-11 02:04:42,425 - INFO - tqdm - accuracy: 0.9713, batch_loss: 0.0119, loss: 0.0838 ||:  76%|#######5  | 857/1131 [04:44<01:20,  3.41it/s]
2022-07-11 02:04:52,495 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.3515, loss: 0.0862 ||:  78%|#######8  | 886/1131 [04:54<01:32,  2.64it/s]
2022-07-11 02:05:02,725 - INFO - tqdm - accuracy: 0.9705, batch_loss: 0.0992, loss: 0.0857 ||:  81%|########  | 915/1131 [05:04<01:21,  2.65it/s]
2022-07-11 02:05:13,152 - INFO - tqdm - accuracy: 0.9701, batch_loss: 0.0189, loss: 0.0868 ||:  84%|########3 | 946/1131 [05:15<01:06,  2.77it/s]
2022-07-11 02:05:23,163 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.0947, loss: 0.0862 ||:  87%|########6 | 981/1131 [05:25<00:39,  3.81it/s]
2022-07-11 02:05:33,290 - INFO - tqdm - accuracy: 0.9702, batch_loss: 0.0205, loss: 0.0863 ||:  89%|########9 | 1011/1131 [05:35<00:41,  2.87it/s]
2022-07-11 02:05:43,490 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.0421, loss: 0.0868 ||:  92%|#########2| 1043/1131 [05:45<00:27,  3.20it/s]
2022-07-11 02:05:53,557 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.0556, loss: 0.0876 ||:  95%|#########4| 1073/1131 [05:55<00:19,  2.96it/s]
2022-07-11 02:06:03,992 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.2443, loss: 0.0878 ||:  98%|#########7| 1105/1131 [06:06<00:10,  2.55it/s]
2022-07-11 02:06:10,073 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0081, loss: 0.0873 ||: 100%|#########9| 1126/1131 [06:12<00:01,  2.82it/s]
2022-07-11 02:06:10,244 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0299, loss: 0.0872 ||: 100%|#########9| 1127/1131 [06:12<00:01,  3.34it/s]
2022-07-11 02:06:10,484 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.2938, loss: 0.0874 ||: 100%|#########9| 1128/1131 [06:12<00:00,  3.55it/s]
2022-07-11 02:06:10,666 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.0064, loss: 0.0873 ||: 100%|#########9| 1129/1131 [06:12<00:00,  3.97it/s]
2022-07-11 02:06:10,906 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0143, loss: 0.0873 ||: 100%|#########9| 1130/1131 [06:13<00:00,  4.03it/s]
2022-07-11 02:06:10,977 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0069, loss: 0.0872 ||: 100%|##########| 1131/1131 [06:13<00:00,  3.03it/s]
2022-07-11 02:06:10,978 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 02:06:10,979 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 02:06:21,024 - INFO - tqdm - accuracy: 0.8137, batch_loss: 0.7009, loss: 0.6295 ||:  42%|####2     | 108/256 [00:10<00:13, 10.68it/s]
2022-07-11 02:06:31,107 - INFO - tqdm - accuracy: 0.8245, batch_loss: 0.0049, loss: 0.6232 ||:  81%|########1 | 208/256 [00:20<00:05,  9.14it/s]
2022-07-11 02:06:35,719 - INFO - tqdm - accuracy: 0.8242, batch_loss: 0.1621, loss: 0.6316 ||: 100%|##########| 256/256 [00:24<00:00, 12.51it/s]
2022-07-11 02:06:35,719 - INFO - tqdm - accuracy: 0.8242, batch_loss: 0.1621, loss: 0.6316 ||: 100%|##########| 256/256 [00:24<00:00, 10.35it/s]
2022-07-11 02:06:35,719 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 02:06:35,720 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.969  |     0.824
2022-07-11 02:06:35,720 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 02:06:35,720 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.087  |     0.632
2022-07-11 02:06:35,720 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 02:06:44,858 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:47.077029
2022-07-11 02:06:44,900 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:33:43
2022-07-11 02:06:44,900 - INFO - allennlp.training.gradient_descent_trainer - Epoch 5/9
2022-07-11 02:06:44,900 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 02:06:44,901 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 02:06:44,902 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 02:06:44,902 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 02:06:54,911 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0022, loss: 0.0476 ||:   3%|2         | 30/1131 [00:10<06:01,  3.04it/s]
2022-07-11 02:07:05,237 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.1547, loss: 0.0393 ||:   5%|5         | 62/1131 [00:20<05:23,  3.31it/s]
2022-07-11 02:07:15,347 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0042, loss: 0.0430 ||:   8%|8         | 93/1131 [00:30<05:27,  3.17it/s]
2022-07-11 02:07:25,619 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0104, loss: 0.0421 ||:  11%|#         | 122/1131 [00:40<05:19,  3.15it/s]
2022-07-11 02:07:35,956 - INFO - tqdm - accuracy: 0.9797, batch_loss: 0.1349, loss: 0.0546 ||:  14%|#3        | 154/1131 [00:51<05:18,  3.07it/s]
2022-07-11 02:07:46,393 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0062, loss: 0.0537 ||:  17%|#6        | 187/1131 [01:01<05:52,  2.68it/s]
2022-07-11 02:07:56,582 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.0090, loss: 0.0554 ||:  19%|#9        | 215/1131 [01:11<05:07,  2.98it/s]
2022-07-11 02:08:06,627 - INFO - tqdm - accuracy: 0.9822, batch_loss: 0.0043, loss: 0.0543 ||:  22%|##1       | 246/1131 [01:21<05:23,  2.73it/s]
2022-07-11 02:08:16,673 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0113, loss: 0.0529 ||:  25%|##4       | 278/1131 [01:31<04:36,  3.09it/s]
2022-07-11 02:08:26,725 - INFO - tqdm - accuracy: 0.9843, batch_loss: 0.0025, loss: 0.0495 ||:  27%|##7       | 310/1131 [01:41<03:43,  3.67it/s]
2022-07-11 02:08:36,833 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0046, loss: 0.0467 ||:  30%|###       | 343/1131 [01:51<04:18,  3.04it/s]
2022-07-11 02:08:46,993 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0043, loss: 0.0510 ||:  33%|###2      | 372/1131 [02:02<05:04,  2.49it/s]
2022-07-11 02:08:57,325 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0040, loss: 0.0491 ||:  36%|###5      | 403/1131 [02:12<04:03,  2.99it/s]
2022-07-11 02:09:07,681 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0017, loss: 0.0482 ||:  39%|###8      | 436/1131 [02:22<04:09,  2.78it/s]
2022-07-11 02:09:17,707 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.7737, loss: 0.0487 ||:  41%|####1     | 465/1131 [02:32<03:59,  2.78it/s]
2022-07-11 02:09:28,043 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0051, loss: 0.0469 ||:  44%|####3     | 496/1131 [02:43<04:07,  2.57it/s]
2022-07-11 02:09:38,137 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0040, loss: 0.0451 ||:  47%|####6     | 529/1131 [02:53<02:50,  3.54it/s]
2022-07-11 02:09:48,313 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0024, loss: 0.0441 ||:  49%|####9     | 558/1131 [03:03<03:39,  2.62it/s]
2022-07-11 02:09:58,550 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0029, loss: 0.0441 ||:  52%|#####2    | 589/1131 [03:13<02:50,  3.18it/s]
2022-07-11 02:10:08,752 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.2935, loss: 0.0488 ||:  55%|#####4    | 620/1131 [03:23<02:57,  2.88it/s]
2022-07-11 02:10:18,899 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0973, loss: 0.0481 ||:  57%|#####7    | 650/1131 [03:33<02:28,  3.24it/s]
2022-07-11 02:10:29,044 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0020, loss: 0.0488 ||:  60%|######    | 683/1131 [03:44<02:09,  3.46it/s]
2022-07-11 02:10:39,306 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0016, loss: 0.0489 ||:  63%|######3   | 716/1131 [03:54<02:21,  2.93it/s]
2022-07-11 02:10:49,578 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0114, loss: 0.0493 ||:  66%|######6   | 749/1131 [04:04<02:24,  2.64it/s]
2022-07-11 02:10:59,723 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0142, loss: 0.0507 ||:  69%|######9   | 782/1131 [04:14<02:07,  2.73it/s]
2022-07-11 02:11:09,953 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0091, loss: 0.0499 ||:  72%|#######1  | 812/1131 [04:25<01:49,  2.91it/s]
2022-07-11 02:11:19,990 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0025, loss: 0.0495 ||:  74%|#######4  | 840/1131 [04:35<01:46,  2.74it/s]
2022-07-11 02:11:30,175 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0642, loss: 0.0497 ||:  77%|#######7  | 872/1131 [04:45<01:16,  3.38it/s]
2022-07-11 02:11:40,300 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0061, loss: 0.0512 ||:  80%|#######9  | 904/1131 [04:55<01:08,  3.32it/s]
2022-07-11 02:11:50,347 - INFO - tqdm - accuracy: 0.9845, batch_loss: 0.0027, loss: 0.0512 ||:  83%|########2 | 935/1131 [05:05<01:01,  3.19it/s]
2022-07-11 02:12:00,424 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0025, loss: 0.0502 ||:  85%|########5 | 964/1131 [05:15<00:52,  3.20it/s]
2022-07-11 02:12:10,622 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0642, loss: 0.0510 ||:  88%|########7 | 993/1131 [05:25<00:51,  2.68it/s]
2022-07-11 02:12:20,695 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0042, loss: 0.0504 ||:  90%|######### | 1022/1131 [05:35<00:41,  2.63it/s]
2022-07-11 02:12:30,984 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.2773, loss: 0.0516 ||:  93%|#########3| 1055/1131 [05:46<00:22,  3.35it/s]
2022-07-11 02:12:41,309 - INFO - tqdm - accuracy: 0.9845, batch_loss: 0.0046, loss: 0.0527 ||:  96%|#########6| 1088/1131 [05:56<00:14,  3.00it/s]
2022-07-11 02:12:51,492 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0145, loss: 0.0519 ||:  99%|#########8| 1119/1131 [06:06<00:03,  3.17it/s]
2022-07-11 02:12:53,510 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.4404, loss: 0.0521 ||: 100%|#########9| 1126/1131 [06:08<00:01,  3.41it/s]
2022-07-11 02:12:53,645 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0061, loss: 0.0521 ||: 100%|#########9| 1127/1131 [06:08<00:00,  4.07it/s]
2022-07-11 02:12:54,087 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0033, loss: 0.0520 ||: 100%|#########9| 1128/1131 [06:09<00:00,  3.28it/s]
2022-07-11 02:12:54,480 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0429, loss: 0.0520 ||: 100%|#########9| 1129/1131 [06:09<00:00,  3.02it/s]
2022-07-11 02:12:54,921 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0025, loss: 0.0520 ||: 100%|#########9| 1130/1131 [06:10<00:00,  2.75it/s]
2022-07-11 02:12:55,004 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0013, loss: 0.0519 ||: 100%|##########| 1131/1131 [06:10<00:00,  3.06it/s]
2022-07-11 02:12:55,004 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 02:12:55,005 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 02:13:05,012 - INFO - tqdm - accuracy: 0.8216, batch_loss: 1.2732, loss: 0.7798 ||:  40%|####      | 103/256 [00:10<00:16,  9.43it/s]
2022-07-11 02:13:15,033 - INFO - tqdm - accuracy: 0.8291, batch_loss: 1.9954, loss: 0.7638 ||:  78%|#######7  | 199/256 [00:20<00:05, 10.56it/s]
2022-07-11 02:13:20,527 - INFO - tqdm - accuracy: 0.8267, batch_loss: 0.7176, loss: 0.7873 ||: 100%|##########| 256/256 [00:25<00:00, 11.46it/s]
2022-07-11 02:13:20,527 - INFO - tqdm - accuracy: 0.8267, batch_loss: 0.7176, loss: 0.7873 ||: 100%|##########| 256/256 [00:25<00:00, 10.03it/s]
2022-07-11 02:13:20,528 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 02:13:20,528 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.985  |     0.827
2022-07-11 02:13:20,528 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 02:13:20,528 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.052  |     0.787
2022-07-11 02:13:20,528 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 02:13:29,760 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:44.857342
2022-07-11 02:13:29,762 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:26:58
2022-07-11 02:13:29,762 - INFO - allennlp.training.gradient_descent_trainer - Epoch 6/9
2022-07-11 02:13:29,762 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 02:13:29,763 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 02:13:29,766 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 02:13:29,766 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 02:13:39,901 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0039, loss: 0.0376 ||:   3%|2         | 29/1131 [00:10<05:09,  3.56it/s]
2022-07-11 02:13:50,302 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0024, loss: 0.0316 ||:   5%|5         | 60/1131 [00:20<07:22,  2.42it/s]
2022-07-11 02:14:00,624 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0019, loss: 0.0475 ||:   8%|8         | 91/1131 [00:30<05:59,  2.89it/s]
2022-07-11 02:14:10,644 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0018, loss: 0.0442 ||:  11%|#         | 121/1131 [00:40<05:19,  3.16it/s]
2022-07-11 02:14:20,747 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0249, loss: 0.0435 ||:  14%|#3        | 154/1131 [00:50<03:37,  4.49it/s]
2022-07-11 02:14:30,822 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0033, loss: 0.0394 ||:  16%|#6        | 185/1131 [01:01<04:18,  3.67it/s]
2022-07-11 02:14:40,846 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0040, loss: 0.0469 ||:  19%|#9        | 216/1131 [01:11<04:24,  3.46it/s]
2022-07-11 02:14:50,869 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0115, loss: 0.0496 ||:  22%|##2       | 250/1131 [01:21<03:42,  3.97it/s]
2022-07-11 02:15:01,137 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.0203, loss: 0.0511 ||:  25%|##5       | 283/1131 [01:31<04:00,  3.53it/s]
2022-07-11 02:15:11,222 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0039, loss: 0.0474 ||:  28%|##7       | 316/1131 [01:41<03:45,  3.62it/s]
2022-07-11 02:15:21,481 - INFO - tqdm - accuracy: 0.9845, batch_loss: 0.0094, loss: 0.0461 ||:  31%|###       | 346/1131 [01:51<04:43,  2.77it/s]
2022-07-11 02:15:31,490 - INFO - tqdm - accuracy: 0.9843, batch_loss: 0.0090, loss: 0.0472 ||:  33%|###3      | 375/1131 [02:01<03:56,  3.19it/s]
2022-07-11 02:15:41,548 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0126, loss: 0.0495 ||:  36%|###5      | 405/1131 [02:11<04:14,  2.86it/s]
2022-07-11 02:15:51,692 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0659, loss: 0.0492 ||:  39%|###8      | 436/1131 [02:21<02:56,  3.93it/s]
2022-07-11 02:16:02,013 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0022, loss: 0.0479 ||:  42%|####1     | 470/1131 [02:32<03:54,  2.82it/s]
2022-07-11 02:16:12,193 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0023, loss: 0.0458 ||:  44%|####3     | 497/1131 [02:42<03:52,  2.73it/s]
2022-07-11 02:16:22,589 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0016, loss: 0.0441 ||:  46%|####6     | 525/1131 [02:52<04:07,  2.45it/s]
2022-07-11 02:16:32,965 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0031, loss: 0.0451 ||:  49%|####9     | 555/1131 [03:03<03:41,  2.60it/s]
2022-07-11 02:16:43,143 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0022, loss: 0.0443 ||:  52%|#####1    | 586/1131 [03:13<03:18,  2.74it/s]
2022-07-11 02:16:53,482 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0072, loss: 0.0480 ||:  55%|#####4    | 618/1131 [03:23<02:40,  3.20it/s]
2022-07-11 02:17:03,914 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0206, loss: 0.0476 ||:  57%|#####7    | 649/1131 [03:34<02:53,  2.77it/s]
2022-07-11 02:17:13,921 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0037, loss: 0.0466 ||:  60%|#####9    | 677/1131 [03:44<02:38,  2.87it/s]
2022-07-11 02:17:24,175 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0011, loss: 0.0447 ||:  63%|######2   | 710/1131 [03:54<02:11,  3.21it/s]
2022-07-11 02:17:34,493 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0081, loss: 0.0459 ||:  66%|######5   | 745/1131 [04:04<01:48,  3.56it/s]
2022-07-11 02:17:44,548 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0046, loss: 0.0461 ||:  69%|######8   | 775/1131 [04:14<01:53,  3.15it/s]
2022-07-11 02:17:54,780 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0089, loss: 0.0467 ||:  71%|#######1  | 804/1131 [04:25<02:05,  2.62it/s]
2022-07-11 02:18:04,941 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0025, loss: 0.0460 ||:  74%|#######4  | 837/1131 [04:35<01:09,  4.22it/s]
2022-07-11 02:18:14,993 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0021, loss: 0.0454 ||:  77%|#######6  | 868/1131 [04:45<00:58,  4.49it/s]
2022-07-11 02:18:25,180 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0091, loss: 0.0467 ||:  80%|#######9  | 900/1131 [04:55<01:17,  2.96it/s]
2022-07-11 02:18:35,560 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0158, loss: 0.0481 ||:  82%|########2 | 932/1131 [05:05<01:16,  2.61it/s]
2022-07-11 02:18:45,842 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0038, loss: 0.0473 ||:  85%|########5 | 967/1131 [05:16<00:51,  3.18it/s]
2022-07-11 02:18:55,900 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0020, loss: 0.0464 ||:  88%|########8 | 997/1131 [05:26<00:33,  3.97it/s]
2022-07-11 02:19:06,146 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.1016, loss: 0.0465 ||:  91%|######### | 1028/1131 [05:36<00:29,  3.45it/s]
2022-07-11 02:19:16,245 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0013, loss: 0.0469 ||:  94%|#########3| 1058/1131 [05:46<00:23,  3.11it/s]
2022-07-11 02:19:26,633 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0017, loss: 0.0466 ||:  96%|#########6| 1090/1131 [05:56<00:13,  3.02it/s]
2022-07-11 02:19:36,711 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0049, loss: 0.0464 ||:  99%|#########9| 1120/1131 [06:06<00:04,  2.47it/s]
2022-07-11 02:19:38,726 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0024, loss: 0.0462 ||: 100%|#########9| 1126/1131 [06:08<00:01,  3.01it/s]
2022-07-11 02:19:39,166 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0029, loss: 0.0462 ||: 100%|#########9| 1127/1131 [06:09<00:01,  2.74it/s]
2022-07-11 02:19:39,455 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0059, loss: 0.0461 ||: 100%|#########9| 1128/1131 [06:09<00:01,  2.92it/s]
2022-07-11 02:19:39,898 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0227, loss: 0.0461 ||: 100%|#########9| 1129/1131 [06:10<00:00,  2.69it/s]
2022-07-11 02:19:40,091 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0019, loss: 0.0461 ||: 100%|#########9| 1130/1131 [06:10<00:00,  3.14it/s]
2022-07-11 02:19:40,181 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0018, loss: 0.0460 ||: 100%|##########| 1131/1131 [06:10<00:00,  3.05it/s]
2022-07-11 02:19:40,182 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 02:19:40,183 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 02:19:50,371 - INFO - tqdm - accuracy: 0.8190, batch_loss: 0.6286, loss: 0.7367 ||:  41%|####1     | 105/256 [00:10<00:14, 10.44it/s]
2022-07-11 02:20:00,478 - INFO - tqdm - accuracy: 0.8219, batch_loss: 0.0204, loss: 0.7474 ||:  83%|########2 | 212/256 [00:20<00:04, 10.50it/s]
2022-07-11 02:20:04,869 - INFO - tqdm - accuracy: 0.8232, batch_loss: 0.1571, loss: 0.7562 ||: 100%|##########| 256/256 [00:24<00:00,  9.78it/s]
2022-07-11 02:20:04,870 - INFO - tqdm - accuracy: 0.8232, batch_loss: 0.1571, loss: 0.7562 ||: 100%|##########| 256/256 [00:24<00:00, 10.37it/s]
2022-07-11 02:20:04,870 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 02:20:04,870 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.986  |     0.823
2022-07-11 02:20:04,871 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 02:20:04,871 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.046  |     0.756
2022-07-11 02:20:04,871 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 02:20:14,085 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:44.323099
2022-07-11 02:20:14,089 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:20:13
2022-07-11 02:20:14,089 - INFO - allennlp.training.gradient_descent_trainer - Epoch 7/9
2022-07-11 02:20:14,089 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 02:20:14,089 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 02:20:14,092 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 02:20:14,092 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 02:20:24,185 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0185, loss: 0.0218 ||:   3%|2         | 32/1131 [00:10<06:07,  2.99it/s]
2022-07-11 02:20:34,574 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0013, loss: 0.0135 ||:   5%|5         | 62/1131 [00:20<06:06,  2.92it/s]
2022-07-11 02:20:44,623 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0026, loss: 0.0235 ||:   8%|8         | 93/1131 [00:30<05:47,  2.99it/s]
2022-07-11 02:20:54,693 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0025, loss: 0.0208 ||:  11%|#         | 121/1131 [00:40<06:04,  2.77it/s]
2022-07-11 02:21:04,902 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0362, loss: 0.0193 ||:  13%|#3        | 151/1131 [00:50<06:04,  2.69it/s]
2022-07-11 02:21:14,942 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0021, loss: 0.0189 ||:  16%|#6        | 182/1131 [01:00<05:08,  3.07it/s]
2022-07-11 02:21:25,132 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0015, loss: 0.0195 ||:  19%|#8        | 214/1131 [01:11<05:17,  2.89it/s]
2022-07-11 02:21:35,235 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0005, loss: 0.0179 ||:  22%|##1       | 246/1131 [01:21<05:16,  2.79it/s]
2022-07-11 02:21:45,660 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0010, loss: 0.0173 ||:  25%|##4       | 278/1131 [01:31<05:49,  2.44it/s]
2022-07-11 02:21:55,703 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0011, loss: 0.0198 ||:  27%|##7       | 310/1131 [01:41<04:09,  3.29it/s]
2022-07-11 02:22:05,767 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0140, loss: 0.0248 ||:  30%|###       | 340/1131 [01:51<04:19,  3.05it/s]
2022-07-11 02:22:16,096 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0068, loss: 0.0266 ||:  33%|###2      | 373/1131 [02:02<04:22,  2.89it/s]
2022-07-11 02:22:26,103 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0215, loss: 0.0252 ||:  36%|###5      | 405/1131 [02:12<03:32,  3.41it/s]
2022-07-11 02:22:36,354 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0034, loss: 0.0253 ||:  38%|###8      | 435/1131 [02:22<04:39,  2.49it/s]
2022-07-11 02:22:46,596 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0012, loss: 0.0238 ||:  41%|####1     | 467/1131 [02:32<03:44,  2.96it/s]
2022-07-11 02:22:56,597 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.7115, loss: 0.0269 ||:  44%|####3     | 497/1131 [02:42<03:10,  3.32it/s]
2022-07-11 02:23:06,727 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0137, loss: 0.0268 ||:  47%|####6     | 528/1131 [02:52<03:35,  2.80it/s]
2022-07-11 02:23:17,064 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0010, loss: 0.0260 ||:  50%|####9     | 561/1131 [03:02<02:49,  3.36it/s]
2022-07-11 02:23:27,383 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.5024, loss: 0.0293 ||:  52%|#####2    | 592/1131 [03:13<02:41,  3.33it/s]
2022-07-11 02:23:37,589 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0070, loss: 0.0307 ||:  55%|#####5    | 625/1131 [03:23<02:54,  2.90it/s]
2022-07-11 02:23:47,665 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0357, loss: 0.0302 ||:  58%|#####8    | 657/1131 [03:33<02:04,  3.82it/s]
2022-07-11 02:23:58,083 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0014, loss: 0.0316 ||:  61%|######    | 687/1131 [03:43<02:31,  2.92it/s]
2022-07-11 02:24:08,359 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0339, loss: 0.0313 ||:  63%|######3   | 718/1131 [03:54<02:13,  3.10it/s]
2022-07-11 02:24:18,484 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0213, loss: 0.0315 ||:  66%|######5   | 746/1131 [04:04<01:57,  3.28it/s]
2022-07-11 02:24:28,588 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0027, loss: 0.0324 ||:  69%|######8   | 780/1131 [04:14<01:45,  3.32it/s]
2022-07-11 02:24:38,962 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0031, loss: 0.0333 ||:  72%|#######1  | 811/1131 [04:24<01:55,  2.76it/s]
2022-07-11 02:24:49,364 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.5272, loss: 0.0339 ||:  75%|#######4  | 844/1131 [04:35<01:47,  2.66it/s]
2022-07-11 02:24:59,398 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0061, loss: 0.0351 ||:  77%|#######7  | 874/1131 [04:45<01:43,  2.49it/s]
2022-07-11 02:25:09,675 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0045, loss: 0.0351 ||:  80%|#######9  | 904/1131 [04:55<01:25,  2.66it/s]
2022-07-11 02:25:19,888 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0014, loss: 0.0352 ||:  83%|########2 | 935/1131 [05:05<01:01,  3.17it/s]
2022-07-11 02:25:29,994 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0015, loss: 0.0352 ||:  85%|########5 | 967/1131 [05:15<00:39,  4.13it/s]
2022-07-11 02:25:40,336 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.4487, loss: 0.0352 ||:  88%|########8 | 999/1131 [05:26<00:53,  2.47it/s]
2022-07-11 02:25:50,637 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0057, loss: 0.0351 ||:  91%|#########1| 1033/1131 [05:36<00:36,  2.70it/s]
2022-07-11 02:26:00,827 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0168, loss: 0.0342 ||:  94%|#########4| 1065/1131 [05:46<00:21,  3.04it/s]
2022-07-11 02:26:10,959 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0242, loss: 0.0335 ||:  97%|#########6| 1095/1131 [05:56<00:10,  3.31it/s]
2022-07-11 02:26:21,155 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0080, loss: 0.0339 ||:  99%|#########9| 1123/1131 [06:07<00:03,  2.60it/s]
2022-07-11 02:26:21,833 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0015, loss: 0.0342 ||: 100%|#########9| 1126/1131 [06:07<00:01,  3.62it/s]
2022-07-11 02:26:22,004 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0046, loss: 0.0342 ||: 100%|#########9| 1127/1131 [06:07<00:00,  4.09it/s]
2022-07-11 02:26:22,190 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0017, loss: 0.0342 ||: 100%|#########9| 1128/1131 [06:08<00:00,  4.41it/s]
2022-07-11 02:26:22,580 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0009, loss: 0.0341 ||: 100%|#########9| 1129/1131 [06:08<00:00,  3.63it/s]
2022-07-11 02:26:22,700 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.1043, loss: 0.0342 ||: 100%|#########9| 1130/1131 [06:08<00:00,  4.37it/s]
2022-07-11 02:26:22,758 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0006, loss: 0.0342 ||: 100%|##########| 1131/1131 [06:08<00:00,  3.07it/s]
2022-07-11 02:26:22,758 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 02:26:22,760 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 02:26:32,879 - INFO - tqdm - accuracy: 0.8325, batch_loss: 0.0143, loss: 0.7384 ||:  41%|####1     | 106/256 [00:10<00:14, 10.13it/s]
2022-07-11 02:26:42,990 - INFO - tqdm - accuracy: 0.8319, batch_loss: 0.1098, loss: 0.7702 ||:  80%|########  | 206/256 [00:20<00:04, 10.76it/s]
2022-07-11 02:26:47,932 - INFO - tqdm - accuracy: 0.8265, batch_loss: 1.1820, loss: 0.8038 ||: 100%|#########9| 255/256 [00:25<00:00,  9.81it/s]
2022-07-11 02:26:48,032 - INFO - tqdm - accuracy: 0.8262, batch_loss: 1.3815, loss: 0.8061 ||: 100%|##########| 256/256 [00:25<00:00, 10.13it/s]
2022-07-11 02:26:48,036 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 02:26:48,036 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.989  |     0.826
2022-07-11 02:26:48,037 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 02:26:48,037 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.034  |     0.806
2022-07-11 02:26:48,037 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 02:26:57,249 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:43.160031
2022-07-11 02:26:57,254 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:13:28
2022-07-11 02:26:57,254 - INFO - allennlp.training.gradient_descent_trainer - Epoch 8/9
2022-07-11 02:26:57,254 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 02:26:57,255 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 02:26:57,256 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 02:26:57,257 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 02:27:07,336 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0020, loss: 0.0229 ||:   2%|2         | 28/1131 [00:10<06:53,  2.67it/s]
2022-07-11 02:27:17,502 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0021, loss: 0.0190 ||:   5%|5         | 59/1131 [00:20<05:10,  3.46it/s]
2022-07-11 02:27:27,547 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.6397, loss: 0.0289 ||:   8%|7         | 90/1131 [00:30<05:43,  3.03it/s]
2022-07-11 02:27:37,693 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0024, loss: 0.0296 ||:  11%|#         | 123/1131 [00:40<05:58,  2.81it/s]
2022-07-11 02:27:47,796 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.1133, loss: 0.0292 ||:  14%|#3        | 153/1131 [00:50<04:39,  3.50it/s]
2022-07-11 02:27:58,007 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0017, loss: 0.0306 ||:  16%|#6        | 183/1131 [01:00<03:52,  4.08it/s]
2022-07-11 02:28:08,225 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0011, loss: 0.0310 ||:  19%|#9        | 215/1131 [01:10<04:57,  3.08it/s]
2022-07-11 02:28:18,495 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0077, loss: 0.0280 ||:  22%|##1       | 245/1131 [01:21<05:06,  2.89it/s]
2022-07-11 02:28:28,735 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.1372, loss: 0.0362 ||:  24%|##4       | 275/1131 [01:31<04:58,  2.87it/s]
2022-07-11 02:28:39,139 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0022, loss: 0.0397 ||:  27%|##7       | 307/1131 [01:41<05:14,  2.62it/s]
2022-07-11 02:28:49,434 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0335, loss: 0.0388 ||:  30%|##9       | 338/1131 [01:52<04:42,  2.80it/s]
2022-07-11 02:28:59,499 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0087, loss: 0.0381 ||:  33%|###2      | 369/1131 [02:02<04:15,  2.98it/s]
2022-07-11 02:29:09,742 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0007, loss: 0.0367 ||:  35%|###5      | 398/1131 [02:12<04:57,  2.47it/s]
2022-07-11 02:29:19,750 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0037, loss: 0.0348 ||:  38%|###8      | 431/1131 [02:22<03:05,  3.78it/s]
2022-07-11 02:29:29,765 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0024, loss: 0.0370 ||:  41%|####      | 462/1131 [02:32<02:52,  3.87it/s]
2022-07-11 02:29:40,197 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0010, loss: 0.0364 ||:  44%|####3     | 494/1131 [02:42<04:06,  2.58it/s]
2022-07-11 02:29:50,259 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0027, loss: 0.0382 ||:  47%|####6     | 526/1131 [02:53<02:59,  3.37it/s]
2022-07-11 02:30:00,470 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0030, loss: 0.0387 ||:  49%|####9     | 558/1131 [03:03<02:58,  3.21it/s]
2022-07-11 02:30:10,662 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0620, loss: 0.0388 ||:  52%|#####2    | 590/1131 [03:13<02:46,  3.26it/s]
2022-07-11 02:30:20,688 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.1476, loss: 0.0377 ||:  55%|#####4    | 619/1131 [03:23<02:52,  2.97it/s]
2022-07-11 02:30:30,806 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0018, loss: 0.0360 ||:  58%|#####7    | 654/1131 [03:33<01:45,  4.50it/s]
2022-07-11 02:30:40,938 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0010, loss: 0.0365 ||:  60%|######    | 682/1131 [03:43<02:30,  2.98it/s]
2022-07-11 02:30:50,983 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0189, loss: 0.0367 ||:  63%|######2   | 711/1131 [03:53<02:52,  2.43it/s]
2022-07-11 02:31:01,333 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0022, loss: 0.0379 ||:  66%|######5   | 744/1131 [04:04<02:17,  2.81it/s]
2022-07-11 02:31:11,537 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0021, loss: 0.0372 ||:  69%|######8   | 775/1131 [04:14<02:03,  2.89it/s]
2022-07-11 02:31:21,904 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0384, loss: 0.0376 ||:  71%|#######1  | 807/1131 [04:24<01:56,  2.78it/s]
2022-07-11 02:31:32,066 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0020, loss: 0.0372 ||:  74%|#######4  | 838/1131 [04:34<01:14,  3.95it/s]
2022-07-11 02:31:42,265 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0038, loss: 0.0367 ||:  77%|#######6  | 869/1131 [04:45<01:29,  2.92it/s]
2022-07-11 02:31:52,270 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0006, loss: 0.0362 ||:  79%|#######9  | 896/1131 [04:55<01:35,  2.45it/s]
2022-07-11 02:32:02,572 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0006, loss: 0.0360 ||:  82%|########1 | 926/1131 [05:05<01:04,  3.18it/s]
2022-07-11 02:32:12,840 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0012, loss: 0.0366 ||:  85%|########4 | 956/1131 [05:15<01:00,  2.91it/s]
2022-07-11 02:32:23,054 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0042, loss: 0.0366 ||:  88%|########7 | 990/1131 [05:25<00:50,  2.77it/s]
2022-07-11 02:32:33,285 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.1091, loss: 0.0369 ||:  91%|######### | 1025/1131 [05:36<00:38,  2.78it/s]
2022-07-11 02:32:43,469 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0025, loss: 0.0370 ||:  93%|#########3| 1057/1131 [05:46<00:22,  3.23it/s]
2022-07-11 02:32:53,658 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.1799, loss: 0.0375 ||:  96%|#########6| 1086/1131 [05:56<00:15,  2.89it/s]
2022-07-11 02:33:03,974 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.2961, loss: 0.0373 ||:  99%|#########8| 1115/1131 [06:06<00:05,  2.83it/s]
2022-07-11 02:33:07,441 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0041, loss: 0.0370 ||: 100%|#########9| 1126/1131 [06:10<00:01,  3.07it/s]
2022-07-11 02:33:07,681 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0076, loss: 0.0370 ||: 100%|#########9| 1127/1131 [06:10<00:01,  3.34it/s]
2022-07-11 02:33:08,122 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0006, loss: 0.0370 ||: 100%|#########9| 1128/1131 [06:10<00:01,  2.92it/s]
2022-07-11 02:33:08,561 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0339, loss: 0.0370 ||: 100%|#########9| 1129/1131 [06:11<00:00,  2.69it/s]
2022-07-11 02:33:09,001 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0035, loss: 0.0370 ||: 100%|#########9| 1130/1131 [06:11<00:00,  2.55it/s]
2022-07-11 02:33:09,145 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0042, loss: 0.0369 ||: 100%|##########| 1131/1131 [06:11<00:00,  3.15it/s]
2022-07-11 02:33:09,146 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0042, loss: 0.0369 ||: 100%|##########| 1131/1131 [06:11<00:00,  3.04it/s]
2022-07-11 02:33:09,146 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 02:33:09,147 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 02:33:19,237 - INFO - tqdm - accuracy: 0.8337, batch_loss: 0.5962, loss: 0.7076 ||:  40%|####      | 103/256 [00:10<00:14, 10.80it/s]
2022-07-11 02:33:29,337 - INFO - tqdm - accuracy: 0.8352, batch_loss: 0.0097, loss: 0.7227 ||:  80%|#######9  | 204/256 [00:20<00:04, 11.10it/s]
2022-07-11 02:33:34,123 - INFO - tqdm - accuracy: 0.8296, batch_loss: 0.9908, loss: 0.7584 ||: 100%|##########| 256/256 [00:24<00:00,  9.94it/s]
2022-07-11 02:33:34,123 - INFO - tqdm - accuracy: 0.8296, batch_loss: 0.9908, loss: 0.7584 ||: 100%|##########| 256/256 [00:24<00:00, 10.25it/s]
2022-07-11 02:33:34,123 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 02:33:34,123 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.987  |     0.830
2022-07-11 02:33:34,124 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 02:33:34,124 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.037  |     0.758
2022-07-11 02:33:34,124 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 02:33:43,360 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:46.103356
2022-07-11 02:33:43,367 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:06:44
2022-07-11 02:33:43,367 - INFO - allennlp.training.gradient_descent_trainer - Epoch 9/9
2022-07-11 02:33:43,367 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.9G
2022-07-11 02:33:43,368 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.6G
2022-07-11 02:33:43,368 - INFO - allennlp.training.gradient_descent_trainer - Training
2022-07-11 02:33:43,370 - INFO - tqdm - 0%|          | 0/1131 [00:00<?, ?it/s]
2022-07-11 02:33:53,396 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0060, loss: 0.0339 ||:   2%|2         | 28/1131 [00:10<06:22,  2.88it/s]
2022-07-11 02:34:03,795 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0008, loss: 0.0181 ||:   5%|5         | 60/1131 [00:20<07:22,  2.42it/s]
2022-07-11 02:34:13,882 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0008, loss: 0.0140 ||:   8%|7         | 89/1131 [00:30<07:24,  2.35it/s]
2022-07-11 02:34:24,008 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0035, loss: 0.0148 ||:  11%|#         | 119/1131 [00:40<06:24,  2.63it/s]
2022-07-11 02:34:34,071 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.4428, loss: 0.0165 ||:  13%|#3        | 150/1131 [00:50<06:03,  2.70it/s]
2022-07-11 02:34:44,157 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0238, loss: 0.0209 ||:  16%|#6        | 182/1131 [01:00<05:42,  2.77it/s]
2022-07-11 02:34:54,224 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0006, loss: 0.0225 ||:  19%|#8        | 213/1131 [01:10<05:00,  3.05it/s]
2022-07-11 02:35:04,317 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0015, loss: 0.0247 ||:  22%|##2       | 249/1131 [01:20<03:06,  4.74it/s]
2022-07-11 02:35:14,486 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0009, loss: 0.0243 ||:  25%|##4       | 280/1131 [01:31<05:16,  2.69it/s]
2022-07-11 02:35:24,769 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0026, loss: 0.0248 ||:  28%|##7       | 313/1131 [01:41<05:11,  2.62it/s]
2022-07-11 02:35:34,814 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0035, loss: 0.0248 ||:  31%|###       | 345/1131 [01:51<03:56,  3.32it/s]
2022-07-11 02:35:44,934 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0484, loss: 0.0234 ||:  33%|###3      | 378/1131 [02:01<03:14,  3.88it/s]
2022-07-11 02:35:55,097 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0056, loss: 0.0228 ||:  36%|###6      | 410/1131 [02:11<03:53,  3.09it/s]
2022-07-11 02:36:05,485 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0007, loss: 0.0233 ||:  39%|###9      | 442/1131 [02:22<03:54,  2.94it/s]
2022-07-11 02:36:15,646 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0012, loss: 0.0240 ||:  42%|####1     | 473/1131 [02:32<03:51,  2.85it/s]
2022-07-11 02:36:25,894 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0012, loss: 0.0229 ||:  44%|####4     | 502/1131 [02:42<03:16,  3.20it/s]
2022-07-11 02:36:36,100 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0120, loss: 0.0225 ||:  47%|####7     | 537/1131 [02:52<03:10,  3.12it/s]
2022-07-11 02:36:46,240 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0007, loss: 0.0231 ||:  50%|#####     | 569/1131 [03:02<02:35,  3.62it/s]
2022-07-11 02:36:56,499 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0034, loss: 0.0247 ||:  53%|#####3    | 601/1131 [03:13<03:04,  2.88it/s]
2022-07-11 02:37:06,702 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0004, loss: 0.0239 ||:  56%|#####5    | 631/1131 [03:23<02:51,  2.92it/s]
2022-07-11 02:37:16,968 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0093, loss: 0.0235 ||:  58%|#####8    | 661/1131 [03:33<02:38,  2.97it/s]
2022-07-11 02:37:27,260 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0005, loss: 0.0237 ||:  61%|######1   | 693/1131 [03:43<02:29,  2.92it/s]
2022-07-11 02:37:37,399 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0134, loss: 0.0251 ||:  64%|######4   | 725/1131 [03:54<02:11,  3.08it/s]
2022-07-11 02:37:47,479 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0091, loss: 0.0248 ||:  67%|######6   | 753/1131 [04:04<02:27,  2.56it/s]
2022-07-11 02:37:57,535 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0003, loss: 0.0246 ||:  69%|######9   | 784/1131 [04:14<01:55,  3.02it/s]
2022-07-11 02:38:07,774 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0005, loss: 0.0241 ||:  72%|#######2  | 816/1131 [04:24<01:42,  3.09it/s]
2022-07-11 02:38:18,211 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0033, loss: 0.0243 ||:  75%|#######4  | 846/1131 [04:34<01:32,  3.07it/s]
2022-07-11 02:38:28,627 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0009, loss: 0.0240 ||:  77%|#######7  | 874/1131 [04:45<01:39,  2.60it/s]
2022-07-11 02:38:38,982 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0011, loss: 0.0236 ||:  80%|#######9  | 903/1131 [04:55<01:31,  2.49it/s]
2022-07-11 02:38:49,081 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0071, loss: 0.0246 ||:  82%|########2 | 932/1131 [05:05<01:07,  2.94it/s]
2022-07-11 02:38:59,335 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0005, loss: 0.0240 ||:  85%|########5 | 962/1131 [05:15<00:54,  3.07it/s]
2022-07-11 02:39:09,759 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0006, loss: 0.0236 ||:  88%|########8 | 996/1131 [05:26<00:44,  3.00it/s]
2022-07-11 02:39:19,968 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0199, loss: 0.0256 ||:  91%|######### | 1027/1131 [05:36<00:30,  3.38it/s]
2022-07-11 02:39:30,022 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.4429, loss: 0.0271 ||:  94%|#########3| 1059/1131 [05:46<00:22,  3.14it/s]
2022-07-11 02:39:40,129 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0092, loss: 0.0270 ||:  96%|#########6| 1090/1131 [05:56<00:16,  2.45it/s]
2022-07-11 02:39:50,407 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0007, loss: 0.0267 ||:  99%|#########9| 1123/1131 [06:07<00:02,  3.24it/s]
2022-07-11 02:39:51,481 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0007, loss: 0.0267 ||: 100%|#########9| 1126/1131 [06:08<00:01,  2.88it/s]
2022-07-11 02:39:51,831 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0039, loss: 0.0266 ||: 100%|#########9| 1127/1131 [06:08<00:01,  2.87it/s]
2022-07-11 02:39:52,134 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0008, loss: 0.0266 ||: 100%|#########9| 1128/1131 [06:08<00:01,  2.99it/s]
2022-07-11 02:39:52,576 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0009, loss: 0.0266 ||: 100%|#########9| 1129/1131 [06:09<00:00,  2.73it/s]
2022-07-11 02:39:52,796 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0011, loss: 0.0266 ||: 100%|#########9| 1130/1131 [06:09<00:00,  3.10it/s]
2022-07-11 02:39:52,870 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0018, loss: 0.0265 ||: 100%|##########| 1131/1131 [06:09<00:00,  3.06it/s]
2022-07-11 02:39:52,871 - INFO - allennlp.training.gradient_descent_trainer - Validating
2022-07-11 02:39:52,872 - INFO - tqdm - 0%|          | 0/256 [00:00<?, ?it/s]
2022-07-11 02:40:03,029 - INFO - tqdm - accuracy: 0.8408, batch_loss: 1.2986, loss: 0.7911 ||:  41%|####1     | 106/256 [00:10<00:16,  9.37it/s]
2022-07-11 02:40:13,203 - INFO - tqdm - accuracy: 0.8367, batch_loss: 0.0237, loss: 0.8397 ||:  82%|########1 | 209/256 [00:20<00:04, 11.34it/s]
2022-07-11 02:40:18,155 - INFO - tqdm - accuracy: 0.8286, batch_loss: 0.9524, loss: 0.8668 ||: 100%|##########| 256/256 [00:25<00:00,  9.21it/s]
2022-07-11 02:40:18,156 - INFO - tqdm - accuracy: 0.8286, batch_loss: 0.9524, loss: 0.8668 ||: 100%|##########| 256/256 [00:25<00:00, 10.13it/s]
2022-07-11 02:40:18,156 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation
2022-07-11 02:40:18,156 - INFO - allennlp.training.callbacks.console_logger - accuracy           |     0.991  |     0.829
2022-07-11 02:40:18,156 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  7829.448  |       N/A
2022-07-11 02:40:18,156 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.027  |     0.867
2022-07-11 02:40:18,157 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8108.699  |       N/A
2022-07-11 02:40:27,365 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:06:43.997509
2022-07-11 02:40:31,381 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 0,
  "peak_worker_0_memory_MB": 8108.69921875,
  "peak_gpu_0_memory_MB": 7829.44775390625,
  "training_duration": "1:07:25.947982",
  "epoch": 9,
  "training_accuracy": 0.9905994249059943,
  "training_loss": 0.026544838080137998,
  "training_worker_0_memory_MB": 8108.69921875,
  "training_gpu_0_memory_MB": 7829.44775390625,
  "validation_accuracy": 0.82861328125,
  "validation_loss": 0.8667617149310445,
  "best_validation_accuracy": 0.82373046875,
  "best_validation_loss": 0.38714570036972873
}
2022-07-11 02:40:31,381 - INFO - allennlp.models.archival - archiving weights and vocabulary to balanced-model-bert-service/model.tar.gz
